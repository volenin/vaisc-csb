{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f99c1ae",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Overview](#Overview)\n",
    "* [Setup](#Setup)\n",
    "    * [Package installs](#Package-installs)\n",
    "    * [Authentication and Google Cloud settings](#Authentication-and-Google-Cloud-settings)\n",
    "    * [Imports](#Imports)\n",
    "    * [Global variables](#Global-variables)\n",
    "    * [Utils](#Utils)\n",
    "    * [Updating searchable and retrievable fields](#Updating-searchable-and-retrievable-fields)\n",
    "* [Data transformation steps](#Data-transformation-steps)\n",
    "    * [Data analysis](#Data-analysis)\n",
    "    * [Simple elements population](#Simple-elements-population)\n",
    "    * [Brand extraction](#Brand-extraction)\n",
    "    * [Description generation](#Description-generation)\n",
    "    * [Tag generation](#Tag-Generation)\n",
    "    * [Color attribute extraction](#Color-attribute-extraction)\n",
    "    * [Create and populate enhanced product Table](#Create-and-Populate-Enhanced-Product-Table)\n",
    "        * [Table cloning and schema extension](#Table-cloning-and-schema-extension)\n",
    "        * [Data population with extended attributes](#Data-population-with-extended-attributes)\n",
    "* [Verification of Data enriched catalog](#Verification-of-Data-enriched-catalog)\n",
    "    * [Data loading](#Data-loading)\n",
    "    * [Validation](#Validation)\n",
    "* [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a53da",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This lab provides a comprehensive guide to managing and enriching product data using Google Cloud services, primarily focusing on Vertex AI and BigQuery. You learn how to perform essential data operations, from initial setup and authentication to advanced data enrichment using generative AI.\n",
    "\n",
    "**Scope:**\n",
    "*   Fetching and analyzing product data from BigQuery.\n",
    "*   Utilizing generative AI (Gemini models) for:\n",
    "    *   Extracting brand names from product titles.\n",
    "    *   Generating compelling product descriptions.\n",
    "    *   Creating relevant product tags.\n",
    "    *   Consolidating and normalizing product tags.\n",
    "    *   Identifying color attributes (color families and specific colors).\n",
    "*   Creating and populating an enhanced product table in BigQuery with the newly generated attributes.\n",
    "*   Verifying the data enrichment process.\n",
    "*   Uploading the enriched product data to product catalog.\n",
    "*   Validating the enriched data through validation tool of Vertex AI Search for commererce.\n",
    "\n",
    "**Key Learnings:**\n",
    "Upon completing this lab, you will be able to:\n",
    "*   Query and analyze product data stored in BigQuery.\n",
    "*   Leverage generative AI models for various product data enrichment tasks, including attribute extraction and content generation.\n",
    "*   Manage and update BigQuery table schemas and data, including creating tables, copying data, and merging updates.\n",
    "*   Understand the practical application of AI in enhancing ecommerce product catalogs.\n",
    "\n",
    "**Steps:**\n",
    "*   **Setup**: You will prepare the Python environment, authenticate with Google Cloud, install required packages, and define global variables and utility functions. This ensures the notebook can interact with Google Cloud services and process data efficiently.\n",
    "*   **Data analysis**: You will analyze the initial product data from BigQuery to understand its current state, schema, and identify missing attributes (e.g., brand, description, tags, detailed color information) that can be enriched.\n",
    "*   **Data enrichment using generative AI**:\n",
    "    *   You will perform preliminary token analysis on product titles to identify potential values for simpler attributes.\n",
    "    *   You will populate `patterns`, `sizes`, and `audience` fields based on the token analysis.\n",
    "    *   You will use the Gemini model to extract `brand` names from product titles.\n",
    "    *   You will generate compelling `description` fields for products using their titles.\n",
    "    *   You will generate product `tags` based on titles and descriptions, then consolidate these tags to create a more refined and manageable set.\n",
    "    *   You will extract `colorInfo` (color families and specific colors) from product titles.\n",
    "*   **Store enriched data in BigQuery**:\n",
    "    *   You will create a new table (`products_enhanced`) in BigQuery by cloning the original product table.\n",
    "    *   You will extend the schema of this new table to include the newly generated attributes.\n",
    "    *   You will populate the new attributes in the `products_enhanced` table using the data generated in the enrichment phase (from the `df_products` DataFrame).\n",
    "*   **Verification**:\n",
    "    *   You will load the enriched product data from the `products_enhanced` BigQuery table into a new branch (Branch 2) of the Vertex AI Search for commerce catalog.\n",
    "    *   You will validate the data enrichment by performing search queries against both the original catalog (Branch 0) and the enriched catalog (Branch 2) using the Vertex AI Search for commerce console, observing differences in search results and facet availability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527aaa36",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Get started by preparing your environment. Begin with authentication and configuration, which will be required for all subsequent API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d36ccb",
   "metadata": {},
   "source": [
    "## Package installs\n",
    "Install all required Python packages. Run this cell only once after starting a new kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7493ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google google-cloud-retail google-cloud-storage google-cloud-bigquery pandas\n",
    "%pip install google-cloud-aiplatform google-genai\n",
    "%pip install google-cloud-bigquery-storage pyarrow tqdm bigquery-magics\n",
    "%pip install google-cloud-bigquery[pandas] jupyterlab\n",
    "%pip install fsspec gcsfs\n",
    "%pip install matplotlib seaborn plotly\n",
    "%pip install --upgrade ipython-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bdc5bc",
   "metadata": {},
   "source": [
    "## Authentication and Google Cloud settings\n",
    "Before you can interact with the Retail API, you must authenticate with Google Cloud and set up your project context. This ensures all API calls are authorized and associated with the correct Google Cloud project. If authentication fails, you'll be prompted to log in interactively. The `project_id` variable will be used throughout the notebook.\n",
    "\n",
    "**About `project_id` and Application Default Credentials (ADC):**\n",
    "\n",
    "- **`project_id`**: This uniquely identifies your Google Cloud project. All API requests, resource creation, and billing are tied to this project. Setting the correct `project_id` ensures your operations are performed in the intended environment and resources are properly tracked.\n",
    "\n",
    "- **Application Default Credentials (ADC)**: ADC is a mechanism that allows your code to automatically find and use your Google Cloud credentials. Running the `gcloud auth application-default login` command sets up ADC by generating credentials that client libraries (like the Retail API) can use to authenticate API calls on your behalf.\n",
    "\n",
    "**Why this matters**  \n",
    "Proper authentication and project selection are essential for secure, authorized access to Google Cloud resources. Without these, API calls will fail or may affect the wrong project. ADC simplifies credential management, especially in development and notebook environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "try:\n",
    "  # Try to get an access token\n",
    "  subprocess.check_output(\n",
    "    ['gcloud', 'auth', 'application-default', 'print-access-token'],\n",
    "    stderr=subprocess.STDOUT\n",
    "  )\n",
    "  print(\"Already authenticated with Application Default Credentials.\")\n",
    "except subprocess.CalledProcessError:\n",
    "  # If it fails, prompt for login\n",
    "  print(\"No valid ADC found. Running interactive login...\")\n",
    "  !gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae0512",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Import all necessary libraries for API access, data analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.retail_v2 import SearchServiceClient, ProductServiceClient, PredictionServiceClient\n",
    "from google.cloud.retail_v2.types import product, search_service, ListProductsRequest, SearchRequest, PredictRequest, UserEvent\n",
    "from google.protobuf.field_mask_pb2 import FieldMask\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "import pandas as pd\n",
    "import http.client as http_client\n",
    "import logging\n",
    "import re\n",
    "from IPython.display import display_html\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# enabling BigQuery magics\n",
    "%load_ext bigquery_magics\n",
    "\n",
    "# configuring default optoins for pandas\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc346d",
   "metadata": {},
   "source": [
    "## Global variables\n",
    "With authentication complete, you define some key variables that will be used in all your API calls. These include resource names and placements, which specify the context for search and recommendation requests.\n",
    "\n",
    "**What is a 'placement'?**  \n",
    "A placement is a configuration resource in the Retail API that determines how and where a model is used for serving search or recommendation results. Placements define the context (such as search, browse, or recommendation) and can be customized for different pages or user experiences.\n",
    "\n",
    "**Why might you have multiple placements or branches?**  \n",
    "- You may have different placements for various parts of your site or app, such as a homepage recommendation carousel, a category browse page, or a personalized search bar.\n",
    "- Multiple branches allow you to manage different versions of your product catalog (e.g., staging vs. production, or A/B testing different product sets).\n",
    "\n",
    "**Example scenarios**\n",
    "- Using a \"default_search\" placement for general product search, and a \"recently_viewed_default\" placement for showing users their recently viewed items.\n",
    "- Having separate branches for testing new product data before rolling it out to all users, or for running experiments with different recommendation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da74112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.auth.exceptions\n",
    "\n",
    "# Authenticate with Google Cloud and get the default project ID\n",
    "try:\n",
    "  credentials, project_id = google.auth.default()\n",
    "  print(f\"Using project ID: {project_id}\")\n",
    "  !gcloud auth application-default set-quota-project {project_id}\n",
    "except google.auth.exceptions.DefaultCredentialsError:\n",
    "  print(\"Google Cloud Authentication failed. Please configure your credentials.\")\n",
    "  print(\"You might need to run 'gcloud auth application-default login'\")\n",
    "  project_id = None # Set to None or a default\n",
    "  \n",
    "SCRIPTS_BUCKET = \"artilekt-vaisc-csb_scripts\"\n",
    "\n",
    "# Define the default placement for search and recommendations\n",
    "DEFAULT_SEARCH = (\n",
    "  f\"projects/{project_id}/locations/global/catalogs/default_catalog/\"\n",
    "  \"placements/default_search\" # Use default_search unless you have a specific browse placement\n",
    ")\n",
    "RECENTLY_VIEWED_DEFAULT = (\n",
    "  f\"projects/{project_id}/locations/global/catalogs/default_catalog/\"\n",
    "  \"placements/recently_viewed_default\"\n",
    ")\n",
    "DEFAULT_BRANCH = f\"projects/{project_id}/locations/global/catalogs/default_catalog/branches/0\"\n",
    "\n",
    "ORIGINAL_PRODUCTS_TABLE = \"retail.products\"\n",
    "ENHANCED_PRODUCTS_TABLE = \"retail.products_enhanced\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658dabf",
   "metadata": {},
   "source": [
    "## Utils\n",
    "To make your analysis easier, you use some utility functions for data conversion and HTTP logging. These help you convert API responses to Pandas DataFrames for analysis and enable detailed logging for troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def json2df(products_list):\n",
    "  if products_list:\n",
    "    products_dicts = [dict(sorted(MessageToDict(p._pb).items())) for p in products_list]\n",
    "    df = pd.json_normalize(products_dicts)\n",
    "    return df\n",
    "  else:\n",
    "    print(\"No products returned or an error occurred.\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def http_logging(log_http: bool):\n",
    "    \"\"\"\n",
    "    Context manager to enable/disable HTTP logging for Google API clients.\n",
    "    Usage:\n",
    "        with http_logging(log_http):\n",
    "            # code that needs HTTP logging\n",
    "    \"\"\"\n",
    "    import http.client as http_client\n",
    "    import logging\n",
    "    root_logger = logging.getLogger()\n",
    "    original_http_debuglevel = http_client.HTTPConnection.debuglevel\n",
    "    original_log_level = root_logger.level\n",
    "    try:\n",
    "        if log_http:\n",
    "            print(\"\\n--- [INFO] Enabling HTTP Logging (forcing REST transport) ---\")\n",
    "            logging.basicConfig()\n",
    "            root_logger.setLevel(logging.DEBUG)\n",
    "            http_client.HTTPConnection.debuglevel = 1\n",
    "            print(\"--- [INFO] Using REST transport. ---\")\n",
    "        yield\n",
    "    finally:\n",
    "        if log_http:\n",
    "            http_client.HTTPConnection.debuglevel = original_http_debuglevel\n",
    "            root_logger.setLevel(original_log_level)\n",
    "            print(\"--- [INFO] HTTP Logging & Root Log Level Restored ---\")\n",
    "\n",
    "from google.cloud.bigquery import SchemaField\n",
    "def flatten_schema_fields(schema, prefix=\"\"):\n",
    "    \"\"\"Flattens the BigQuery schema, expanding RECORD types.\"\"\"\n",
    "    fields_data = []\n",
    "    for field in schema:\n",
    "        field_name_full = f\"{prefix}{field.name}\"\n",
    "        if field.field_type == 'RECORD' and field.fields:\n",
    "            fields_data.append({\n",
    "                \"Field Name\": field_name_full,\n",
    "                \"Data Type\": field.field_type, # or \"STRUCT\"\n",
    "                \"Mode\": field.mode\n",
    "            })\n",
    "            fields_data.extend(flatten_schema_fields(field.fields, prefix=f\"{field_name_full}.\"))\n",
    "        else:\n",
    "            fields_data.append({\n",
    "                \"Field Name\": field_name_full,\n",
    "                \"Data Type\": field.field_type,\n",
    "                \"Mode\": field.mode\n",
    "            })\n",
    "    return fields_data\n",
    "\n",
    "\n",
    "# Initialize Vertex AI\n",
    "from google import genai\n",
    "import json\n",
    "\n",
    "def gemini_run_query(contents, model_name=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Runs a query against a specified Gemini model and expects a JSON response.\n",
    "\n",
    "    This function initializes a new genai.Client for each call, configured\n",
    "    with the global `project_id` and \"us-central1\" location. It sends the\n",
    "    provided `contents` to the specified `model_name`. The model is expected\n",
    "    to return a response in JSON format.\n",
    "\n",
    "    Args:\n",
    "        contents: The prompt or content to send to the Gemini model.\n",
    "                  This should be structured according to the model's requirements.\n",
    "        model_name: The name of the Gemini model to use (e.g., \"gemini-2.0-flash\").\n",
    "                    Defaults to \"gemini-2.0-flash\".\n",
    "        \n",
    "    Returns:\n",
    "        A Python dictionary parsed from the model's JSON response if successful.\n",
    "        Returns None if an error occurs during the API call or JSON parsing.\n",
    "    \"\"\"\n",
    "    # Default config for JSON response\n",
    "    effective_config = {\"response_mime_type\": \"application/json\"}\n",
    "\n",
    "    # Initialize the Gemini client. \n",
    "    # Note: `project_id` should be globally defined and available in this scope.\n",
    "    gemini = genai.Client(project=project_id, location=\"us-central1\", vertexai=True)\n",
    "\n",
    "    try:\n",
    "        response = gemini.models.generate_content( \n",
    "            model=model_name, \n",
    "            contents=contents,\n",
    "            config=effective_config,\n",
    "        )\n",
    "        # Parse the JSON response\n",
    "        respText = response.text.strip()\n",
    "        # print(f\"Response text: {respText}\") # Uncomment for debugging model responses\n",
    "        # Convert response to JSON\n",
    "        return json.loads(respText)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in gemini_run_query (model: {model_name}, contents starting with: '{str(contents)[:100]}...'): {e}\")\n",
    "        return None # Return None on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244ba56",
   "metadata": {},
   "source": [
    "## Updating searchable and retrievable fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5d2c9",
   "metadata": {},
   "source": [
    "The Retail API allows you to specify which fields in your product catalog are searchable, indexable, and retrievable. Attribute configuration settings will impact search and recommendations behavior across your site.\n",
    "\n",
    "Search uses the following attribute settings:\n",
    "\n",
    "* **Indexable:** Search can filter and facet using this attribute.\n",
    "* **Dynamic faceting:** Search can automatically use this attribute as a dynamic facet based on past user behavior such as facet clicks and views. To enable dynamic faceting for an attribute, Indexable must be set to true for that attribute.\n",
    "* **Searchable:** This attribute is searchable by search queries, which increases recall for that attribute. This control is applicable only for text attributes.\n",
    "* **Retrievable:** If set to true, search returns this attribute in responses to search queries. If all attributes have Retrievable set to false, the search results contain only the product name or (for variants) the product name and color information.\n",
    "\n",
    "It is recommended that you retrieve only the fields you need for your application to optimize performance and reduce costs. For the purpose of this lab, you make all fields searchable and retrievable. This is not recommended for production use, but it will simplify demonstration of the effects some of the advanced search configurations have on the results.\n",
    "\n",
    "Note that changes to indexable, searchable, and retrievable take effect immediately upon your next catalog full ingestion or within 12 hours or more. Changes to dynamic faceting and tiling and exact match take effect within 2 days. Changes to filterable only apply to filter tag generation for recommendations and take effect within 12 hours or more.\n",
    "\n",
    "You explore this configuration and effects of these settings in another lab. For the purpose of this lab, set all attributes to searchable and retrievable. This allows you to see the effects of the generative AI enrichment on the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20242c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from google.cloud.retail_v2.types import CatalogAttribute, AttributesConfig, UpdateAttributesConfigRequest\n",
    "from google.cloud.retail_v2 import CatalogServiceClient\n",
    "\n",
    "catalog_service_client = CatalogServiceClient()\n",
    "catalog_name = f\"projects/{project_id}/locations/global/catalogs/default_catalog/attributesConfig\"\n",
    "attributes_config = catalog_service_client.get_attributes_config(name=catalog_name)\n",
    "\n",
    "# Allowed predefined textual attributes for searchable_option\n",
    "ALLOWED_SEARCHABLE_PREDEFINED = {\n",
    "    \"ageGroups\", \"brands\", \"categories\", \"colorFamilies\", \"conditions\",\n",
    "    \"description\", \"genders\", \"materials\", \"patterns\", \"sizes\", \"title\"\n",
    "}\n",
    "\n",
    "# Define a list of important attributes to be made retrievable (max 30, must be allowed by Retail API)\n",
    "IMPORTANT_RETRIEVABLE_ATTRIBUTES = [\n",
    "  \"ageGroups\",\n",
    "  \"availability\",\n",
    "  \"brands\",\n",
    "  \"categories\",\n",
    "  \"colorFamilies\",\n",
    "  \"colors\",\n",
    "  \"conditions\",\n",
    "  \"cost\",\n",
    "  \"currencyCode\",\n",
    "  \"description\",\n",
    "  \"discount\",\n",
    "  \"genders\",\n",
    "  \"gtin\",\n",
    "  \"images\",\n",
    "  \"materials\",\n",
    "  \"name\",\n",
    "  \"originalPrice\",\n",
    "  \"patterns\",\n",
    "  \"price\",\n",
    "  \"productId\",\n",
    "  \"rating\",\n",
    "  \"ratingCount\",\n",
    "  \"ratingHistogram\",\n",
    "  \"sizes\",\n",
    "  \"title\",\n",
    "  \"uri\",\n",
    "  # Custom attributes below (keep at the bottom)\n",
    "  \"attributes.collection\",\n",
    "  \"attributes.ecofriendly\",\n",
    "  \"attributes.material\",\n",
    "  \"attributes.style\",\n",
    "]\n",
    "\n",
    "for attr_name, attr in attributes_config.catalog_attributes.items():\n",
    "    attr.indexable_option = CatalogAttribute.IndexableOption.INDEXABLE_ENABLED\n",
    "    \n",
    "    # Set retrievable_option based on importance and inventory status\n",
    "    if attr_name.startswith(\"inventories.\"):\n",
    "        attr.retrievable_option = CatalogAttribute.RetrievableOption.RETRIEVABLE_DISABLED\n",
    "    elif attr_name in IMPORTANT_RETRIEVABLE_ATTRIBUTES:\n",
    "        attr.retrievable_option = CatalogAttribute.RetrievableOption.RETRIEVABLE_ENABLED\n",
    "    else:\n",
    "        attr.retrievable_option = CatalogAttribute.RetrievableOption.RETRIEVABLE_DISABLED\n",
    "        \n",
    "    # Only set searchable_option=ENABLED for allowed attributes\n",
    "    if attr_name in ALLOWED_SEARCHABLE_PREDEFINED:\n",
    "        attr.searchable_option = CatalogAttribute.SearchableOption.SEARCHABLE_ENABLED\n",
    "    elif attr_name.startswith(\"attributes.\") and attr.type_ == CatalogAttribute.AttributeType.TEXTUAL:\n",
    "        attr.searchable_option = CatalogAttribute.SearchableOption.SEARCHABLE_ENABLED\n",
    "    else:\n",
    "        attr.searchable_option = CatalogAttribute.SearchableOption.SEARCHABLE_DISABLED\n",
    "\n",
    "# Prepare update request\n",
    "update_req = UpdateAttributesConfigRequest(\n",
    "    attributes_config=attributes_config,\n",
    "    update_mask=None  # None means update all fields\n",
    ")\n",
    "\n",
    "# Update the config in the API\n",
    "updated_config = catalog_service_client.update_attributes_config(request=update_req)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294d196",
   "metadata": {},
   "source": [
    "Now that the attributes have been updated, purge the catalog Branch 0 to ensure that the changes take effect immediately. This removes all existing products from the index and trigger a full reindexing with the new attribute settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.retail_v2 import ProductServiceClient\n",
    "from google.cloud.retail_v2.types import PurgeProductsRequest\n",
    "\n",
    "# Use the existing product_service_client and project_id variables\n",
    "branch = DEFAULT_BRANCH\n",
    "\n",
    "purge_request = PurgeProductsRequest(\n",
    "  parent=branch,\n",
    "  filter='*',  # Purge all products\n",
    "  force=True   # Actually perform the purge (not a dry run)\n",
    ")\n",
    "\n",
    "product_service_client = ProductServiceClient()\n",
    "operation = product_service_client.purge_products(request=purge_request)\n",
    "print(\"Purging catalog... This may take a few minutes.\")\n",
    "result = operation.result()\n",
    "print(\"Purge operation completed.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48a224",
   "metadata": {},
   "source": [
    "> **⚠️ It may take 2-5 minutes for the background process to reload the catalog.** You do not need to wait for this process to complete before proceeding with the rest of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4cff39",
   "metadata": {},
   "source": [
    "# Data transformation steps\n",
    "You now proceed with the data transformation steps. \n",
    "- First, you analyze the existing product data loaded into Branch 0 of the product catalog. You identify any gaps or issues in the data. \n",
    "- Then you see how to address these gaps using data enrichment techniques powered by generative AI models.\n",
    "- Finally, you create a new table in BigQuery with the enriched product data and validate it using the Vertex AI Search for commerce validation tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8de36",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "Analyze the existing product data loaded into Branch 0 of the product catalog. The data has been imported from a BigQuery table 'products' under 'retail' dataset. Here is the schema and a sample of the data. You can also view the data in the [BigQuery Console](https://console.cloud.google.com/bigquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeaf674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd # Ensure pandas is imported for DataFrame creation\n",
    "\n",
    "client = bigquery.Client(project=project_id) \n",
    "\n",
    "TABLE_ID = f\"{project_id}.retail.products\"\n",
    "\n",
    "try:\n",
    "    table = client.get_table(TABLE_ID)  # Make an API request.\n",
    "    print(f\"Schema for table {TABLE_ID}:\")\n",
    "\n",
    "    # Prepare data for DataFrame using the new flattening function\n",
    "    schema_data = flatten_schema_fields(table.schema)\n",
    "    \n",
    "    df_schema = pd.DataFrame(schema_data)\n",
    "    display(df_schema)\n",
    "    \n",
    "    print(f\"\\n\\nSample data from {TABLE_ID} (first 50 rows):\")\n",
    "\n",
    "\n",
    "\n",
    "    # Query to fetch product titles and IDs\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM `{project_id}.retail.products`\n",
    "    WHERE title IS NOT NULL\n",
    "    LIMIT 50\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Fetching product data from {project_id}.retail.products...\")\n",
    "    sample_products = client.query(query).to_dataframe()\n",
    "    print(f\"Fetched {len(sample_products)} products\")\n",
    "    display(sample_products)\n",
    "\n",
    "\n",
    "    # Populate df_products DataFrame with distinct product titles. \n",
    "    # This DataFrame will be used later to add AI-generated information back to the products.\n",
    "    print(f\"Fetching distinct product titles from {project_id}.retail.products...\")\n",
    "     # Query to fetch distinct product titles\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        DISTINCT title\n",
    "    FROM `{project_id}.retail.products`\n",
    "    WHERE title IS NOT NULL\n",
    "    \"\"\"\n",
    "    df_products = client.query(query).to_dataframe()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching schema or data for table {TABLE_ID}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a548ed",
   "metadata": {},
   "source": [
    "Some important fields which impact ranking and searchability of the products are not populated (such as description). You might also notice that some fields are missing entirely from this schema, such as brand and tags. Here is the full schema describing all possible fields that can be populated in the product catalog. (You can also view the data in the [BigQuery Console](https://console.cloud.google.com/bigquery), retail.products_tmpl table.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd # Ensure pandas is imported for DataFrame creation\n",
    "\n",
    "client = bigquery.Client(project=project_id) \n",
    "\n",
    "TABLE_ID_TMPL = f\"{project_id}.retail.products_tmpl\"\n",
    "\n",
    "try:\n",
    "    table_tmpl = client.get_table(TABLE_ID_TMPL)  # Make an API request.\n",
    "    print(f\"Schema for table {TABLE_ID_TMPL}:\")\n",
    "\n",
    "    # Prepare data for DataFrame using the new flattening function\n",
    "    schema_data_tmpl = flatten_schema_fields(table_tmpl.schema)\n",
    "    \n",
    "    df_schema_tmpl = pd.DataFrame(schema_data_tmpl)\n",
    "    display(df_schema_tmpl)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching schema or data for table {TABLE_ID_TMPL}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc64f20",
   "metadata": {},
   "source": [
    "Even though `id,` `title` and `category` are the only mandatory fields needed to load catalog data, it is highly desirable to populate as many additional fields as possible to improve the searchability and ranking of the products. Some are more important than others, such as `description`, `brand`, `tags`, `color families`, `target audience`.\n",
    "\n",
    "If you look at the fields in the original product data, you will notice that the `title` field is the most informative one, which might contain useful information for use to be able to populate additional fields for at least some of the products. There is also a valid reference to `image` uri. In this lab, you focus on working with text fields due to simplicity and low requirement of additional data sources. Various ways to leverage `image` data will be discussed at the end of the lab.\n",
    "\n",
    "Analyze tokens in the `title` field and see if you can categorize them against the fields you want to populate. The best approach to analyze unstructured text data is to use generative AI models, which can help you extract structured information from the text. You use the Gemini model for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# The 'gemini_run_query' function (defined in an earlier cell, e.g., Cell 13) \n",
    "# initializes its own Gemini client. No separate client initialization is needed here for it.\n",
    "\n",
    "# BigQuery client\n",
    "client_bq = bigquery.Client(project=project_id) \n",
    "\n",
    "# 1. Fetch product titles\n",
    "print(\"Fetching product titles...\")\n",
    "query_titles = f\"SELECT DISTINCT title FROM `{project_id}.retail.products` WHERE title IS NOT NULL\"\n",
    "df_titles = client_bq.query(query_titles).to_dataframe()\n",
    "print(f\"Fetched {len(df_titles)} titles.\")\n",
    "\n",
    "# 2. Fetch target categories from products_tmpl schema\n",
    "# Assuming df_schema_tmpl is already defined and populated from a previous cell (cell 17)\n",
    "print(\"\\nUsing pre-existing categories from df_schema_tmpl (from cell 17)...\")\n",
    "target_categories = df_schema_tmpl['Field Name'].tolist()\n",
    "# Filter out categories that are too generic or complex for simple token mapping if needed\n",
    "# This list can be refined based on desired output\n",
    "target_categories = [\n",
    "    cat for cat in target_categories \n",
    "    if not ('.' in cat and not any(sub in cat for sub in ['attributes', 'audience', 'colorInfo', 'priceInfo', 'fulfillmentInfo', 'promotion'])) and \n",
    "       cat not in ['name', 'id', 'uri', 'title', 'categories', 'primaryProductId', 'description', 'languageCode', 'gtin', 'retrievableFields', 'expireTime', 'ttl', 'collectionId', 'image']\n",
    "]\n",
    "print(f\"Using {len(target_categories)} target categories for token mapping: {target_categories}\")\n",
    "\n",
    "# 3. Tokenize titles\n",
    "print(\"\\nTokenizing titles...\")\n",
    "all_tokens_list = [] \n",
    "for title_text in df_titles['title']:\n",
    "    # Simple tokenization: lowercase and split by non-alphanumeric characters, keeping words\n",
    "    tokens = re.findall(r'\\b\\w+\\b', str(title_text).lower())\n",
    "    all_tokens_list.extend(tokens)\n",
    "\n",
    "unique_tokens_list = list(set(all_tokens_list))\n",
    "print(f\"Found {len(all_tokens_list)} total tokens, {len(unique_tokens_list)} unique tokens.\")\n",
    "\n",
    "# 4. Categorize tokens with Gemini\n",
    "def categorize_tokens_with_ai(tokens_batch, categories_list):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in e-commerce product data categorization.\n",
    "    Given a list of product attribute names (categories) and a list of tokens (words from product titles),\n",
    "    your task is to map each token to the categories from the provided list.\n",
    "\n",
    "    Categories to use for mapping:\n",
    "    {json.dumps(categories_list)}\n",
    "\n",
    "    Rules:\n",
    "    1. For each token, identify the best matching categories from the list.\n",
    "    2. A token can belong to multiple categories.\n",
    "    3. If a token does not clearly fit into any of the provided categories, categorize it as \"Uncategorized\".\n",
    "    4. Return the result as a JSON map where the key is the token and the value is the assigned category string.\n",
    "    5. Be precise. For example, if categories include \"attributes.brand\" and \"attributes.color\", a token like \"Nike\" should map to \"attributes.brand\", and \"red\" to \"attributes.color\".\n",
    "    6. If a token seems like a general descriptor not fitting a specific attribute (e.g., \"new\", \"sale\", \"for\"), map it to \"Uncategorized\".\n",
    "\n",
    "    Tokens to categorize:\n",
    "    {json.dumps(tokens_batch)}\n",
    "    \"\"\"\n",
    "    # Call the generic gemini_run_query function\n",
    "    categorized_map = gemini_run_query(\n",
    "            contents=prompt,\n",
    "\n",
    "    )\n",
    "\n",
    "    if categorized_map is None: # Error occurred in gemini_run_query\n",
    "            # gemini_run_query already prints a detailed error.\n",
    "            # This function needs to return the specific error structure it promised.\n",
    "            print(f\"Error categorizing batch (tokens starting with '{tokens_batch[0] if tokens_batch else ''}'). AI query failed.\")\n",
    "            return {token: \"Error\" for token in tokens_batch}\n",
    "\n",
    "    return categorized_map\n",
    "\n",
    "print(\"\\nCategorizing tokens with Gemini...\")\n",
    "batch_size_categorization = 150 # Adjusted batch size to manage prompt length and complexity\n",
    "master_token_category_map = {}\n",
    "\n",
    "if unique_tokens_list:\n",
    "    for i in tqdm(range(0, len(unique_tokens_list), batch_size_categorization)):\n",
    "        current_tokens_batch = unique_tokens_list[i:i+batch_size_categorization]\n",
    "        batch_map = categorize_tokens_with_ai(current_tokens_batch, target_categories)\n",
    "        master_token_category_map.update(batch_map) # batch_map will be {token:\"Error\",...} or {} on error, or valid map\n",
    "        time.sleep(1.5) # Increased sleep time slightly for API rate limits with potentially larger prompts\n",
    "else:\n",
    "    print(\"No unique tokens to categorize.\")\n",
    "\n",
    "print(f\"Finished categorizing tokens. {len(master_token_category_map)} unique tokens processed.\")\n",
    "\n",
    "# 5. Aggregate results\n",
    "print(\"\\nAggregating results...\")\n",
    "# Initialize with all target categories plus Uncategorized and Error\n",
    "category_token_counts = {cat: Counter() for cat in target_categories + [\"Uncategorized\", \"Error\"]}\n",
    "\n",
    "for token_item in all_tokens_list: # Iterate through all tokens (with duplicates) to get correct counts\n",
    "    # Get the category for the token, defaulting to \"Uncategorized\" if not in map or if mapping failed\n",
    "    category_for_token = master_token_category_map.get(token_item, \"Uncategorized\")\n",
    "    \n",
    "    # Ensure the category is one you are tracking, otherwise default to Uncategorized\n",
    "    if category_for_token not in category_token_counts:\n",
    "        # This case handles if AI returns a category not in target_categories or if it's \"Error\" from categorize_tokens_with_ai\n",
    "        if category_for_token == \"Error\": # Explicitly count tokens that resulted in an \"Error\" category\n",
    "            category_token_counts[\"Error\"][token_item] +=1\n",
    "            continue # Skip further processing for this error token\n",
    "        else: # For any other unexpected category, map to \"Uncategorized\"\n",
    "            category_for_token = \"Uncategorized\"\n",
    "        \n",
    "    category_token_counts[category_for_token][token_item] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ce264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Format output\n",
    "print(\"\\nSummary of token categorization:\")\n",
    "summary_data_list = [] # Renamed to avoid conflict\n",
    "for category_name, token_counter_obj in category_token_counts.items():\n",
    "    if not token_counter_obj: # Skip categories with no tokens assigned\n",
    "        continue\n",
    "    total_tokens_in_category = sum(token_counter_obj.values())\n",
    "    # Get (token, count) for top 50, then extract just the token string\n",
    "    tokens_list = [item[0] for item in token_counter_obj.most_common(50)]\n",
    "    summary_data_list.append({\n",
    "        \"category\": category_name,\n",
    "        \"token_count\": total_tokens_in_category,\n",
    "        \"tokens_list\": tokens_list\n",
    "    })\n",
    "\n",
    "# Sort by token count in descending order\n",
    "df_summary_output = pd.DataFrame(summary_data_list)\n",
    "if not df_summary_output.empty:\n",
    "    df_summary_output = df_summary_output.sort_values(by=\"token_count\", ascending=False).reset_index(drop=True)\n",
    "    print(\"Category, Token Count, [Top 50 Tokens]\")\n",
    "    display(df_summary_output)\n",
    "else:\n",
    "    print(\"No data to summarize.\")\n",
    "\n",
    "print(\"\\nToken analysis and categorization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d10c8",
   "metadata": {},
   "source": [
    "Out of the categorized tokens, some can be ignored (such as 'type, uncategorized), while others can be used to populate the fields you want to enrich. For example, tokens 'Nike', 'Adidas', 'Puma' can be used to populate the `brand` field, while tokens like 'running', 'sports', 'casual' can be used to populate the `tags` field. Field types that seem to contain useful information are:\n",
    "- `colorInfo.colors`\n",
    "- `brands`\n",
    "- `audience`\n",
    "- `materials`\n",
    "- `patterns`\n",
    "- `sizes`\n",
    "\n",
    "Now that you understand the existing product data and the fields you want to enrich, proceed with the data enrichment steps. You use generative AI models to extract structured information from the `title` field and populate some of the fields in the product catalog:\n",
    "- `colorInfo`\n",
    "- `brands`\n",
    "- `description`\n",
    "- `tags`\n",
    "\n",
    "But first, you populate some of the fields where values seem to have been identified correctly by preliminary token based analysis. These fields are:\n",
    "- `patterns`\n",
    "- `sizes`\n",
    "- `audience`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe952d2",
   "metadata": {},
   "source": [
    "## Simple elements population\n",
    "\n",
    "By initial analysis of the product titles, you have already extracted some useful information that can be used to populate the `patterns`, `sizes`, and `audience` fields. You now populate them using simple string matching techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print tokens for patterns, sizes, audiences\n",
    "display(df_summary_output[df_summary_output['category'].str.contains('pattern|size|audience.', regex=True, case=False)])\n",
    "\n",
    "# populate patterns, sizes and audiences in df_products if any word in the tokens_list is found in the title\n",
    "\n",
    "# Helper function to get all tokens for a given category keyword\n",
    "def get_tokens_for_category(df_summary, category_keyword):\n",
    "    relevant_rows = df_summary[df_summary['category'].str.contains(category_keyword, regex=True, case=False)]\n",
    "    all_tokens = set()\n",
    "    for tokens_list in relevant_rows['tokens_list']:\n",
    "        for token in tokens_list:\n",
    "            all_tokens.add(str(token).lower())\n",
    "    return list(all_tokens)\n",
    "\n",
    "# Get dynamic lists of tokens\n",
    "pattern_tokens = get_tokens_for_category(df_summary_output, 'pattern')\n",
    "size_tokens = get_tokens_for_category(df_summary_output, 'size')\n",
    "# For audience, you might want to be more specific if 'audience.' matches too broadly\n",
    "# or combine 'audience.genders' and 'audience.ageGroups'\n",
    "audience_gender_tokens = get_tokens_for_category(df_summary_output, 'audience.genders')\n",
    "audience_age_tokens = get_tokens_for_category(df_summary_output, 'audience.ageGroups')\n",
    "audience_tokens = {\n",
    "    \"genders\": audience_gender_tokens,\n",
    "    \"ageGroups\": audience_age_tokens\n",
    "}\n",
    "\n",
    "print(f\"Pattern tokens: {pattern_tokens}\")\n",
    "print(f\"Size tokens: {size_tokens}\")\n",
    "print(f\"Audience tokens: {audience_tokens}\")\n",
    "\n",
    "def extract_patterns(title):\n",
    "    patterns = []\n",
    "    # Ensure pattern_tokens is not empty before proceeding\n",
    "    if not pattern_tokens:\n",
    "        return None\n",
    "    for token in title.split():\n",
    "        if token.lower() in pattern_tokens:\n",
    "            patterns.append(token)\n",
    "    return ', '.join(patterns) if patterns else None\n",
    "\n",
    "def extract_sizes(title):\n",
    "    sizes = []\n",
    "    # Ensure size_tokens is not empty\n",
    "    if not size_tokens:\n",
    "        return None\n",
    "    for token in title.split():\n",
    "        if token.lower() in size_tokens:\n",
    "            sizes.append(token)\n",
    "    return ', '.join(sizes) if sizes else None\n",
    "\n",
    "def extract_audiences(title):\n",
    "    \"\"\"Extract audience information including genders and ageGroups from title\"\"\"\n",
    "    result = {\n",
    "        \"genders\": [],\n",
    "        \"ageGroups\": []\n",
    "    }\n",
    "    \n",
    "    # Ensure audience_tokens is not empty and has expected structure\n",
    "    if not audience_tokens or not isinstance(audience_tokens, dict):\n",
    "        return None\n",
    "    \n",
    "    title_tokens = [token.lower() for token in title.split()]\n",
    "    \n",
    "    # Check for gender tokens\n",
    "    if \"genders\" in audience_tokens and audience_tokens[\"genders\"]:\n",
    "        for token in title_tokens:\n",
    "            if token in audience_tokens[\"genders\"]:\n",
    "                result[\"genders\"].append(token)\n",
    "    \n",
    "    # Check for age group tokens\n",
    "    if \"ageGroups\" in audience_tokens and audience_tokens[\"ageGroups\"]:\n",
    "        for token in title_tokens:\n",
    "            if token in audience_tokens[\"ageGroups\"]:\n",
    "                result[\"ageGroups\"].append(token)\n",
    "    \n",
    "    # Return None if no audience data found, otherwise return the structured result\n",
    "    if not result[\"genders\"] and not result[\"ageGroups\"]:\n",
    "        return None\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "df_products['pattern'] = df_products['title'].apply(lambda x: extract_patterns(x))\n",
    "df_products['size'] = df_products['title'].apply(lambda x: extract_sizes(x))\n",
    "df_products['audience'] = df_products['title'].apply(lambda x: extract_audiences(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c02246",
   "metadata": {},
   "source": [
    "Let's validate the results and display few products with populated fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1907559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(df_products[\n",
    "    df_products['pattern'].notna() | \n",
    "    df_products['size'].notna() | \n",
    "    df_products['audience'].notna()\n",
    "][['title', 'pattern', 'size', 'audience']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16a932",
   "metadata": {},
   "source": [
    "## Brand extraction\n",
    "\n",
    "Now you use Vertex AI's generative AI capabilities to extract brand information from product titles. You use the Gemini model to analyze titles and identify potential brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def extract_brands_with_ai(titles_batch, suggested_brands):\n",
    "    \"\"\"\n",
    "    Extract brands from a batch of product titles using generative AI,\n",
    "    leveraging a list of suggested brands from prior tokenization.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create prompt for brand extraction\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in e-commerce product data analysis. Your task is to extract brand names from product titles.\n",
    "\n",
    "    Consider these potential brand names, which were identified during an initial analysis of product titles. Use them as strong suggestions:\n",
    "    {json.dumps(suggested_brands)}\n",
    "\n",
    "    Rules:\n",
    "    1. Extract only actual brand names, not product types or categories.\n",
    "    2. Use the provided list of potential brand names as strong suggestions. However, also identify other brands if they are clearly present in the title but not on the suggestion list.\n",
    "    3. Return results as a JSON map (NOT array of maps) where the key is the original title and the value is an array of identified brand names (e.g., {{\"title\": [\"BrandA\", \"BrandB\"]}}).\n",
    "    4. If a title contains a single brand, return that brand as a single element within an array (e.g., {{\"title\": [\"BrandA\"]}}).\n",
    "    5. If no brand is identifiable in a title, return \"Unknown\" as a single element in an array for that title (e.g., {{\"title\": [\"Unknown\"]}}).\n",
    "    6. If a title contains multiple brands, return all identified brands in an array, in the order they appear in the title (e.g., {{\"title\": [\"BrandX\", \"BrandY\"]}}).\n",
    "    7. Be consistent with brand name formatting. If a brand matches one from the suggested list, prefer the casing from that list. Otherwise, use standard capitalization (e.g., \"Google\", not \"google\").\n",
    "    8. Ignore generic terms, product types, or attributes that are not brand names.\n",
    "    \n",
    "    Product titles to analyze:\n",
    "    {chr(10).join([f\"{i+1}. {title}\" for i, title in enumerate(titles_batch)])}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # print(prompt) # Uncomment for debugging the prompt\n",
    "        # Call the gemini_run_query function\n",
    "        brands_map = gemini_run_query(\n",
    "            contents=prompt,\n",
    "        )\n",
    "        # print(f\"brands: {brands_map}\") # Uncomment for debugging the raw response from Gemini\n",
    "        if brands_map is None: # Error occurred in gemini_run_query\n",
    "            # gemini_run_query already prints a detailed error.\n",
    "            # This function needs to return the specific error structure it promised.\n",
    "            print(f\"AI query failed for titles starting with '{titles_batch[0] if titles_batch else 'N/A'}'.\")\n",
    "            return {title: [\"Error\"] for title in titles_batch}\n",
    "            \n",
    "        return brands_map\n",
    "    except Exception as e: # Catch any other unexpected errors during the process\n",
    "        print(f\"Unexpected error in extract_brands_with_ai for titles starting with '{titles_batch[0] if titles_batch else 'N/A'}': {e}\")\n",
    "        return {title: [\"Error\"] for title in titles_batch}\n",
    "\n",
    "# Process titles in batches\n",
    "print(\"=== Extracting Brands using generative AI ===\")\n",
    "\n",
    "# Prepare suggested brands from earlier tokenization (cell 18)\n",
    "suggested_brands_from_tokenization = []\n",
    "if 'category_token_counts' in globals() and isinstance(category_token_counts, dict) and 'brands' in category_token_counts:\n",
    "    suggested_brands_from_tokenization = list(category_token_counts['brands'].keys())\n",
    "    print(f\"Using {len(suggested_brands_from_tokenization)} suggested brands from tokenization analysis.\")\n",
    "    # Optional: Display a sample of suggested brands\n",
    "    # print(f\"Sample of suggested brands: {suggested_brands_from_tokenization[:10]}\")\n",
    "else:\n",
    "    print(\"Warning: 'category_token_counts' not found or 'brands' key missing. Proceeding without brand suggestions for the AI.\")\n",
    "\n",
    "\n",
    "batch_size = 200  # Small batch size to avoid API limits\n",
    "final_merged_brands_map = {} # To store all brand maps from all batches\n",
    "\n",
    "# Get unique titles for processing\n",
    "unique_titles = df_products['title'].unique().tolist()\n",
    "print(f\"Processing {len(unique_titles)} unique product titles in batches of {batch_size}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(unique_titles), batch_size)):\n",
    "    print(f\"Processing batch {i // batch_size + 1}...\")\n",
    "    current_batch_titles = unique_titles[i:i+batch_size]\n",
    "    \n",
    "    # extract_brands_with_ai is expected to return a dict: {title: [brands], ...}\n",
    "    brands_data_for_batch = extract_brands_with_ai(current_batch_titles, suggested_brands_from_tokenization)\n",
    "    \n",
    "    # Update the main dictionary with data from the current batch\n",
    "    if isinstance(brands_data_for_batch, dict):\n",
    "        final_merged_brands_map.update(brands_data_for_batch)\n",
    "    else:\n",
    "        # Fallback if extract_brands_with_ai doesn't return a dict\n",
    "        print(f\"Warning: AI processing for batch starting with '{current_batch_titles[0] if current_batch_titles else 'N/A'}' did not return a dictionary. These titles may be missing brand data or have errors.\")\n",
    "        for title_in_batch in current_batch_titles:\n",
    "            final_merged_brands_map.setdefault(title_in_batch, [\"Error - Batch Data Invalid\"])\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "# Assign the extracted brands to the DataFrame\n",
    "# Map titles to their extracted brand lists; titles not in map get NaN\n",
    "df_products['brand'] = df_products['title'].map(final_merged_brands_map)\n",
    "\n",
    "# Ensure that the 'brand' column contains lists.\n",
    "# If map resulted in NaN (title not found) or if value is not a list, set to [\"Unknown\"].\n",
    "df_products['brand'] = df_products['brand'].apply(\n",
    "    lambda x: x if isinstance(x, list) else [\"Unknown\"]\n",
    ")\n",
    "print(\"Brand extraction completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7c193",
   "metadata": {},
   "source": [
    "Let's display the 'title' and the extracted 'brand' for the first 10 products in the `df_products` DataFrame to show the results of the brand extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0303aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_products[['title', 'brand']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e38ff",
   "metadata": {},
   "source": [
    "## Description generation\n",
    "Next, you generate product descriptions using the Gemini model. This helps you create compelling and informative descriptions for each product based on its title and other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure client is initialized (it should be from the previous cell, but good for clarity)\n",
    "# client = genai.Client(project=project_id, location=\"us-central1\", vertexai=True)\n",
    "\n",
    "def generate_descriptions_with_ai(titles_batch):\n",
    "    \"\"\"\n",
    "    Generate product descriptions from a batch of product titles using generative AI.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert e-commerce copywriter. Your task is to generate compelling and concise product descriptions \n",
    "    suitable for a retail portal based on product titles.\n",
    "\n",
    "    Rules:\n",
    "    1. For each product title, generate a description that is 2-4 sentences long.\n",
    "    2. Highlight key features and benefits implied by the title.\n",
    "    3. Use engaging and persuasive language.\n",
    "    4. Return results as a JSON map (NOT as an array of maps) where the key is the original product title and the value is the generated product description string.\n",
    "    5. If a product title is too generic or uninformative to generate a meaningful description, use title as description.\n",
    "    6. Ensure the output is a valid JSON object.\n",
    "\n",
    "    Product titles to analyze:\n",
    "    {chr(10).join([f\"{i+1}. {title}\" for i, title in enumerate(titles_batch)])}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Call the gemini_run_query function, which handles model selection,\n",
    "        # JSON response type, and basic error handling.\n",
    "        descriptions_map = gemini_run_query(contents=prompt)\n",
    "\n",
    "        if descriptions_map is None: # Error occurred in gemini_run_query\n",
    "            # gemini_run_query already prints a detailed error.\n",
    "            # This function needs to return the specific error structure it promised.\n",
    "            print(f\"AI query failed for titles starting with '{titles_batch[0] if titles_batch else 'N/A'}'.\")\n",
    "            return {title: \"Error generating description\" for title in titles_batch}\n",
    "            \n",
    "        return descriptions_map\n",
    "    except Exception as e: # Catch any other unexpected errors during the process\n",
    "        print(f\"Unexpected error in generate_descriptions_with_ai for titles starting with '{titles_batch[0] if titles_batch else 'N/A'}': {e}\")\n",
    "        return {title: \"Error generating description\" for title in titles_batch}\n",
    "\n",
    "# Process titles for descriptions in batches\n",
    "print(\"\\n=== Generating Product Descriptions using generative AI ===\")\n",
    "batch_size = 100  # Adjust batch size as needed, considering prompt length and API limits for descriptions\n",
    "final_merged_descriptions_map = {}\n",
    "\n",
    "# Get unique titles for processing\n",
    "unique_titles_for_description = df_products['title'].unique().tolist()\n",
    "print(f\"Processing {len(unique_titles_for_description)} unique product titles for descriptions in batches of {batch_size}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(unique_titles_for_description), batch_size)):\n",
    "    print(f\"Processing description batch {i // batch_size + 1}...\")\n",
    "    current_batch_titles = unique_titles_for_description[i:i+batch_size]\n",
    "    \n",
    "    descriptions_data_for_batch = generate_descriptions_with_ai(current_batch_titles)\n",
    "    \n",
    "    if isinstance(descriptions_data_for_batch, dict):\n",
    "        final_merged_descriptions_map.update(descriptions_data_for_batch)\n",
    "    else:\n",
    "        print(f\"Warning: AI processing for description batch starting with '{current_batch_titles[0] if current_batch_titles else 'N/A'}' did not return a dictionary.\")\n",
    "        for title_in_batch in current_batch_titles:\n",
    "            final_merged_descriptions_map.setdefault(title_in_batch, \"Error - Batch Data Invalid\")\n",
    "    \n",
    "    time.sleep(1) # Respect API rate limits\n",
    "\n",
    "# Assign the generated descriptions to the DataFrame\n",
    "df_products['description'] = df_products['title'].map(final_merged_descriptions_map)\n",
    "\n",
    "# Ensure that the 'description' column contains strings, default if not.\n",
    "df_products['description'] = df_products['description'].apply(\n",
    "    lambda x: x if isinstance(x, str) else \"Description not available\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119187c",
   "metadata": {},
   "source": [
    "Let's take a look at the first 10 products in the `df_products` DataFrame to see the generated descriptions alongside their titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e901bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProduct description generation completed!\")\n",
    "display(df_products[['title', 'description']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd81b04",
   "metadata": {},
   "source": [
    "## Tag generation\n",
    "Now, you generate product tags using the Gemini model. Tags are essential for improving searchability and categorization of products in the catalog. You use the model to create relevant tags based on product titles and descriptions. You also incorporate the brand information extracted earlier to ensure tags are relevant and comprehensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffa3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re # Ensure re is imported\n",
    "\n",
    "# Ensure client is initialized\n",
    "# client = genai.Client(project=project_id, location=\"us-central1\", vertexai=True)\n",
    "\n",
    "def generate_tags_with_ai(batch_data, suggested_tags):\n",
    "    \"\"\"\n",
    "    Generate tags from a batch of product titles, descriptions, and brands using generative AI.\n",
    "    Brands (provided in batch_data) are removed from the generated tags during post-processing.\n",
    "    Suggested tags are provided to the AI as a hint.\n",
    "    Batch data should be a list of dictionaries, each with 'title', 'description', and 'brand'.\n",
    "    \"\"\"\n",
    "\n",
    "    tag_rules = \"\"\"\n",
    "      Tag best practices:\n",
    "      1. Tag Length: The ideal length for a tag is typically 1 to 3 words.\n",
    "\n",
    "      The goal is to capture the essence of a searchable keyword or a specific attribute. Think about what a user would realistically type into a search bar or what would make sense as a clickable filter in a sidebar.\n",
    "\n",
    "      Good Examples (1-3 words):\n",
    "      - organic cotton\n",
    "      - water-resistant\n",
    "      - summer collection\n",
    "      - vintage wash\n",
    "      - limited edition\n",
    "      - graphic print\n",
    "\n",
    "\n",
    "      2. What to Avoid:\n",
    "      - Too Broad (often 1 word): A tag like \"shirt\" is often too generic if the product is already in a \"Shirts\" category. It adds very little new information.\n",
    "      - Too Long (like a sentence): A tag like \"this t-shirt is made from premium sustainable cotton\" is not a tag; it's a description. It's ineffective for faceting and won't match user search behavior.\n",
    "    \"\"\"\n",
    "    prompt_parts = [\n",
    "        \"You are an expert ecommerce product tagger. Your task is to generate a list of 5 to 10 relevant tags for each product based on its title and description.\",\n",
    "        \"Focus on keywords that customers might use to search for these products.\",\n",
    "        \"Tags should be concise and can include product attributes, category, use cases, or key features.\",\n",
    "        tag_rules,\n",
    "        \"Consider these potential tags, which were identified during an initial analysis. Use them as strong suggestions if applicable, but also generate other relevant tags:\",\n",
    "        json.dumps(suggested_tags),\n",
    "        \"Return results as a JSON map where the key is the original product title and the value is an array of generated tag strings.\",\n",
    "        \"If a product title or description is too generic or uninformative to generate meaningful tags, return an empty array for that title.\",\n",
    "        \"Ensure the output is a valid JSON object.\",\n",
    "        \"\\nProduct data to analyze:\"\n",
    "    ]\n",
    "    \n",
    "    for i, item in enumerate(batch_data):\n",
    "        prompt_parts.append(f\"{i+1}. Title: {item['title']}\\n   Description: {item['description']}\")\n",
    "        \n",
    "    prompt = \"\\n\".join(prompt_parts)\n",
    "\n",
    "    try:\n",
    "        raw_tags_map = gemini_run_query(\n",
    "                    contents=prompt,\n",
    "        )\n",
    "\n",
    "        # Post-process to remove brands from tags\n",
    "        final_tags_map = {}\n",
    "        for item_in_batch in batch_data: # Iterate through the input batch_data to get brand info\n",
    "            title = item_in_batch['title']\n",
    "            product_brands = item_in_batch.get('brand', []) \n",
    "            \n",
    "            if not isinstance(product_brands, list):\n",
    "                product_brands = [] \n",
    "\n",
    "            # Filter out \"Unknown\" or \"Error\" brands, convert to lowercase, and remove empty strings\n",
    "            valid_brands_lower = [\n",
    "                b.lower().strip() for b in product_brands \n",
    "                if isinstance(b, str) and b.lower().strip() not in [\"unknown\", \"error\", \"\"]\n",
    "            ]\n",
    "            # Sort by length descending to remove longer brand names first (e.g., \"google pixel\" before \"google\")\n",
    "            valid_brands_lower.sort(key=len, reverse=True)\n",
    "            \n",
    "            ai_generated_tags = raw_tags_map.get(title, [])\n",
    "            if not isinstance(ai_generated_tags, list): # Ensure AI output for title is a list\n",
    "                ai_generated_tags = []\n",
    "\n",
    "            cleaned_product_tags = set() # Use set to avoid duplicate tags after cleaning\n",
    "            for tag_str in ai_generated_tags:\n",
    "                if not isinstance(tag_str, str) or not tag_str.strip(): # Skip if tag is not a string or empty\n",
    "                    continue\n",
    "                \n",
    "                modified_tag_str = tag_str.lower() # Work with lowercase tag\n",
    "\n",
    "                for brand_lower in valid_brands_lower:\n",
    "                    if not brand_lower: # Skip empty brand strings from the sorted list\n",
    "                        continue\n",
    "                    # Remove brand, ensuring whole word match. re.escape handles special chars in brand names.\n",
    "                    modified_tag_str = re.sub(f'\\b{re.escape(brand_lower)}\\b', '', modified_tag_str, flags=re.IGNORECASE)\n",
    "                \n",
    "                # Clean up extra spaces that might result from removal and normalize multiple spaces to one\n",
    "                modified_tag_str = ' '.join(modified_tag_str.split()).strip()\n",
    "\n",
    "                if modified_tag_str: # Add tag only if it's not empty after brand removal\n",
    "                    cleaned_product_tags.add(modified_tag_str)\n",
    "            \n",
    "            final_tags_map[title] = sorted(list(cleaned_product_tags)) # Store unique, sorted tags\n",
    "\n",
    "        return final_tags_map\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating tags for batch starting with '{batch_data[0]['title'] if batch_data else 'N/A'}': {e}\")\n",
    "        # Return a map with empty tag lists for all items in the batch in case of error\n",
    "        return {item['title']: [] for item in batch_data}\n",
    "\n",
    "# Prepare data for tag generation: unique title-description-brand sets\n",
    "# Ensure 'brand' column exists from previous brand extraction cell\n",
    "# Use drop_duplicates on title, assuming brand and description are dependent on title for uniqueness in this context.\n",
    "df_unique_products_for_tags = df_products[['title', 'description', 'brand']].drop_duplicates(subset=['title']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Generating Product Tags using generative AI (brands will be excluded) ===\")\n",
    "\n",
    "# Prepare suggested tags from earlier tokenization (cell 18)\n",
    "suggested_tags_from_tokenization = []\n",
    "if 'category_token_counts' in globals() and isinstance(category_token_counts, dict) and 'tags' in category_token_counts:\n",
    "    suggested_tags_from_tokenization = list(category_token_counts['tags'].keys())\n",
    "    print(f\"Using {len(suggested_tags_from_tokenization)} suggested tags from tokenization analysis.\")\n",
    "    # Optional: Display a sample of suggested tags\n",
    "    # print(f\"Sample of suggested tags: {suggested_tags_from_tokenization[:20]}\")\n",
    "else:\n",
    "    print(\"Warning: 'category_token_counts' not found or 'tags' key missing. Proceeding without tag suggestions for the AI.\")\n",
    "\n",
    "\n",
    "batch_size_tags = 100  # Adjust based on typical length of title + description and token limits\n",
    "final_merged_tags_map = {}\n",
    "\n",
    "print(f\"Processing {len(df_unique_products_for_tags)} unique title/description/brand combinations for tags in batches of {batch_size_tags}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(df_unique_products_for_tags), batch_size_tags)):\n",
    "    print(f\"Processing tags batch {i // batch_size_tags + 1}...\")\n",
    "    current_batch_data = []\n",
    "    for idx, row in df_unique_products_for_tags.iloc[i:i+batch_size_tags].iterrows():\n",
    "        # Include brand in the data passed to the AI function for post-processing\n",
    "        current_batch_data.append({'title': row['title'], \n",
    "                                   'description': row['description'], \n",
    "                                   'brand': row['brand']}) # Add brand here\n",
    "    \n",
    "    if not current_batch_data:\n",
    "        continue\n",
    "        \n",
    "    # Pass suggested_tags_from_tokenization to the function\n",
    "    tags_data_for_batch = generate_tags_with_ai(current_batch_data, suggested_tags_from_tokenization)\n",
    "    \n",
    "    if isinstance(tags_data_for_batch, dict):\n",
    "        final_merged_tags_map.update(tags_data_for_batch)\n",
    "    else:\n",
    "        print(f\"Warning: AI processing for tags batch did not return a dictionary.\")\n",
    "        # Default to empty list for titles in this problematic batch\n",
    "        for item in current_batch_data:\n",
    "            final_merged_tags_map.setdefault(item['title'], []) \n",
    "    \n",
    "    time.sleep(1) # Respect API rate limits\n",
    "\n",
    "# Assign the generated tags to the DataFrame\n",
    "df_products['tags'] = df_products['title'].map(final_merged_tags_map)\n",
    "\n",
    "# Ensure that the 'tags' column contains lists, default to empty list if not found or not a list.\n",
    "df_products['tags'] = df_products['tags'].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0882aad1",
   "metadata": {},
   "source": [
    "Let's check few products to see how `tags` were generated based on the product titles and descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nProduct tag generation completed! (Brands excluded from tags)\")\n",
    "display(df_products[['title', 'tags']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539f7ef",
   "metadata": {},
   "source": [
    "Let's analyze the results to see how well the model performs in identifying brands from product titles. Look for patterns in the extracted brand names and assess the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n=== Analyzing Product Tags ===\")\n",
    "\n",
    "# Ensure all tags are strings and convert to lowercase\n",
    "df_products['tags'] = df_products['tags'].apply(\n",
    "    lambda tag_list: [str(tag).lower() for tag in tag_list if isinstance(tag, str) or not pd.isna(tag)] if isinstance(tag_list, list) else []\n",
    ")\n",
    "\n",
    "# Explode the tags list into separate rows for individual tag analysis\n",
    "all_tags = df_products['tags'].explode().dropna() # dropna to remove any NaN from explode if a product had an empty list initially\n",
    "\n",
    "# Calculate tag frequencies\n",
    "tag_counts = all_tags.value_counts()\n",
    "\n",
    "print(f\"Total unique tags (after lowercase): {len(tag_counts)}\")\n",
    "print(\"\\nTop 20 most common tags:\")\n",
    "display(tag_counts.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab54033",
   "metadata": {},
   "source": [
    "Let's see if you can generalize some tags to avoid 'tag swamp' and improve searchability. Consolidate tags by grouping similar ones together, ensuring that the final set of tags is both comprehensive and relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2597b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure client is initialized (it should be from a previous cell)\n",
    "# client = genai.Client(project=project_id, location=\"us-central1\", vertexai=True)\n",
    "\n",
    "def consolidate_tags_with_ai(tags_batch):\n",
    "    \"\"\"\n",
    "    Consolidate a batch of tags using generative AI.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert data normalizer specializing in e-commerce product tags.\n",
    "    Your task is to consolidate a list of product tags to reduce variability and group semantically similar tags under a single canonical form.\n",
    "    Aim to reduce the overall number of unique tags significantly, towards a target of 50-100 unique tags.\n",
    "\n",
    "    Rules:\n",
    "    1. Analyze the provided list of tags.\n",
    "    2. Identify groups of tags that are synonyms, misspellings, plural/singular variations, or closely related concepts.\n",
    "    3. For each group, suggest a single, concise, and representative canonical tag.\n",
    "    4. If a tag is already in a good canonical form or is unique and distinct, it can map to itself.\n",
    "    5. Prioritize broader terms if multiple specific terms can be grouped (e.g., \"hdmi cable,\" \"usb cable\" could remain distinct or be grouped under \"cables\" if very aggressive consolidation is needed, but for now, try to keep meaningful distinctions unless they are very minor variations).\n",
    "    6. Ensure the canonical tag is in lowercase.\n",
    "    7. Remove the tags that are too generic or not useful for search (e.g., \"product\", \"item\", \"thing\", \"location\") unless they are part of a meaningful tag.\n",
    "    8. Try to generate tags that are non-overlapping and distinct, avoiding redundancy.\n",
    "    9. When you generate an initial list of canonical tags, ensure they are distinct and meaningful, perform another iteration of analysis based on the rules above, to avoid redundancy.\n",
    "    10. Return the results as a JSON map where the key is the original tag from the input batch and the value is its suggested canonical tag.\n",
    "\n",
    "    Example:\n",
    "    Input Tags: [\"running shoe\", \"Running Shoes\", \"runningshoe\", \"sneaker\", \"sporty shoe\", \"athletic footwear\", \"trainer\"]\n",
    "    Output JSON Map: {{\n",
    "        \"running shoe\": \"running shoe\",\n",
    "        \"Running Shoes\": \"running shoe\",\n",
    "        \"runningshoe\": \"running shoe\",\n",
    "        \"runner\": \"running shoe\",\n",
    "        \"white sneakers\": \"sneaker\"\n",
    "        \"sneaker\": \"sneaker\",\n",
    "        \"sporty shoe\": \"sneaker\",\n",
    "        \"athletic footwear\": \"athletic shoe\",\n",
    "        \"trainer shoes\": \"athletic shoe\"\n",
    "    }}\n",
    "    (Note: This example is illustrative. Your grouping might be different based on the overall list and desired consolidation level.)\n",
    "\n",
    "    Tags to consolidate:\n",
    "    {json.dumps(tags_batch)}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        consolidation_map = gemini_run_query(\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return consolidation_map\n",
    "    except Exception as e:\n",
    "        print(f\"Error consolidating tags for batch: {e}\")\n",
    "        # Fallback: return a map where each tag maps to itself\n",
    "        return {tag: tag for tag in tags_batch}\n",
    "\n",
    "print(\"\\n=== Consolidating Product Tags using generative AI ===\")\n",
    "\n",
    "# Ensure tags are lowercase first (assuming previous cell was run)\n",
    "df_products['tags'] = df_products['tags'].apply(\n",
    "    lambda tag_list: [str(tag).lower() for tag in tag_list if isinstance(tag, str) or not pd.isna(tag)] if isinstance(tag_list, list) else []\n",
    ")\n",
    "all_lowercase_tags = df_products['tags'].explode().dropna().unique().tolist()\n",
    "\n",
    "print(f\"Found {len(all_lowercase_tags)} unique lowercase tags to consolidate.\")\n",
    "\n",
    "batch_size_consolidation = 500 # Adjust based on typical number of tags and prompt limits\n",
    "master_consolidation_map = {}\n",
    "\n",
    "if all_lowercase_tags:\n",
    "    for i in tqdm(range(0, len(all_lowercase_tags), batch_size_consolidation)):\n",
    "        print(f\"Processing consolidation batch {i // batch_size_consolidation + 1}...\")\n",
    "        current_tags_batch = all_lowercase_tags[i:i+batch_size_consolidation]\n",
    "        \n",
    "        batch_map = consolidate_tags_with_ai(current_tags_batch)\n",
    "        master_consolidation_map.update(batch_map)\n",
    "        \n",
    "        time.sleep(1) # Respect API rate limits\n",
    "else:\n",
    "    print(\"No tags found to consolidate.\")\n",
    "\n",
    "print(f\"Master consolidation map created with {len(master_consolidation_map)} entries.\")\n",
    "\n",
    "def apply_tag_consolidation(tag_list, consolidation_map):\n",
    "    if not isinstance(tag_list, list):\n",
    "        return []\n",
    "    consolidated_tags = set() # Use a set to store unique canonical tags for the product\n",
    "    for tag in tag_list:\n",
    "        tag_lower = str(tag).lower() # Ensure it's lowercase, though it should be already\n",
    "        canonical_tag = consolidation_map.get(tag_lower, tag_lower) # Get from map, or use original if not found\n",
    "        consolidated_tags.add(canonical_tag)\n",
    "    return sorted(list(consolidated_tags))\n",
    "\n",
    "# Apply the consolidation map to the 'tags' column\n",
    "if master_consolidation_map:\n",
    "    df_products['tags'] = df_products['tags'].apply(lambda x: apply_tag_consolidation(x, master_consolidation_map))\n",
    "    print(\"\\nProduct tags consolidated.\")\n",
    "    # Calculate and print the number of unique tags after consolidation\n",
    "    all_consolidated_tags = df_products['tags'].explode().dropna().unique().tolist()\n",
    "    print(f\"Total unique tags after consolidation: {len(all_consolidated_tags)}\")\n",
    "else:\n",
    "    print(\"\\nNo consolidation map created, tags remain unchanged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906989bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of tags before consolidation:\", len(all_lowercase_tags))\n",
    "print(\"number of tags after consolidation:\", len(df_products['tags'].explode().dropna().unique().tolist()))\n",
    "\n",
    "# Calculate and display top 20 consolidated tags with counts\n",
    "consolidated_tag_counts = df_products['tags'].explode().dropna().value_counts()\n",
    "\n",
    "# --- New: Side-by-side comparison of top 20 initial and consolidated tags ---\n",
    "print(\"\\n=== Comparison: Top 20 Initial vs. Consolidated Tags ===\")\n",
    "\n",
    "# Get top 20 initial tags (assuming 'tag_counts' is available from Cell 27)\n",
    "if 'tag_counts' in globals():\n",
    "    top_initial_tags = tag_counts.head(20).reset_index()\n",
    "    top_initial_tags.columns = ['Initial Tag', 'Initial Count']\n",
    "else:\n",
    "    print(\"Warning: 'tag_counts' (initial tags) not found. Skipping initial tags display.\")\n",
    "    # Create an empty DataFrame with expected columns if tag_counts is not available\n",
    "    top_initial_tags = pd.DataFrame(columns=['Initial Tag', 'Initial Count'])\n",
    "\n",
    "\n",
    "top_consolidated_tags = consolidated_tag_counts.head(20).reset_index()\n",
    "top_consolidated_tags.columns = ['Consolidated Tag', 'Consolidated Count']\n",
    "\n",
    "# Combine for side-by-side display\n",
    "# Use pd.concat. If the number of rows is different, it will fill with NaN.\n",
    "# For a cleaner look, ensure both DataFrames have the same number of rows (e.g., by taking head(20) from both)\n",
    "# or decide on a primary key if a merge is more appropriate (not the case here for simple top N display).\n",
    "\n",
    "# Reset index for both to allow concatenation by position if lengths are same\n",
    "# If lengths might differ and you want to align by rank:\n",
    "df_comparison = pd.concat([top_initial_tags, top_consolidated_tags], axis=1)\n",
    "\n",
    "# Fill NaN values that might appear if one list is shorter than the other (though both are head(20))\n",
    "df_comparison = df_comparison.fillna('') # Fill with empty string for display\n",
    "\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88275e22",
   "metadata": {},
   "source": [
    "You did pretty good, significantly reducing the space of tags. Here is a sample of `tags` field after consolidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6deae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_products[['title', 'tags']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b359245",
   "metadata": {},
   "source": [
    "## Color attribute extraction\n",
    "Now, for the grand finale, you extract color attributes from the product titles. You use the Gemini model to identify color families and specific colors associated with each product. This helps you enhance the product catalog with detailed color information. Note that you also could have analyzed the images referred in the `image` field, but for simplicity you focus on text data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c29841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure the genai client is initialized (it should be from a previous cell)\n",
    "# client = genai.Client(project=project_id, location=\"us-central1\", vertexai=True)\n",
    "\n",
    "def generate_color_attributes_with_ai(titles_batch):\n",
    "    \"\"\"\n",
    "    Generate colorFamilies and colors from a batch of product titles using generative AI.\n",
    "    titles_batch: A list of product titles.\n",
    "    \"\"\"\n",
    "    prompt_parts = [\n",
    "        \"You are an expert in e-commerce product data analysis. Your task is to extract color families and specific colors from product information.\",\n",
    "        \"Based on the provided product title, identify for each product:\", # Changed to title only\n",
    "        \"1.  `colorFamilies`: A list of standard color groups.\",\n",
    "        \"    *   Strongly recommended to use only these values: \\\"Red\\\", \\\"Pink\\\", \\\"Orange\\\", \\\"Yellow\\\", \\\"Purple\\\", \\\"Green\\\", \\\"Cyan\\\", \\\"Blue\\\", \\\"Brown\\\", \\\"White\\\", \\\"Gray\\\", \\\"Black\\\", \\\"Mixed\\\".\",\n",
    "        \"    *   Normally, provide only 1 color family. If multiple distinct color families are clearly present and necessary, list them, but prefer \\\"Mixed\\\" if appropriate.\",\n",
    "        \"    *   Maximum of 5 values allowed. Each value must be a UTF-8 encoded string, max 128 characters.\",\n",
    "        \"    *   If no color is identifiable from the title, return an empty list for this key.\", # Clarified source\n",
    "        \"2.  `colors`: A list of color display names (e.g., frontend aliases like \\\"scarlet red\\\", \\\"ocean blue\\\").\",\n",
    "        \"    *   Normally, provide only 1 color. If multiple distinct colors are clearly present and necessary, list them.\",\n",
    "        \"    *   Maximum of 75 colors allowed. Each value must be a UTF-8 encoded string, max 128 characters.\",\n",
    "        \"    *   If no color is identifiable from the title, return an empty list for this key.\", # Clarified source\n",
    "        \"\",\n",
    "        \"Return results as a single JSON map where the key is the original product title, and the value is a JSON object with two keys: \\\"colorFamilies\\\" (a list of strings) and \\\"colors\\\" (a list of strings).\",\n",
    "        \"Example for a single product entry in the map:\",\n",
    "        \"\\\"Product Title Example\\\": { \\\"colorFamilies\\\": [\\\"Red\\\"], \\\"colors\\\": [\\\"Scarlet Red\\\", \\\"Crimson\\\"] }\",\n",
    "        \"If no colors can be determined for a product from its title, its entry should be like:\", # Clarified source\n",
    "        \"\\\"Another Product Title\\\": { \\\"colorFamilies\\\": [], \\\"colors\\\": [] }\",\n",
    "        \"Ensure the entire output is a valid JSON object (a single map).\",\n",
    "        \"\\nProduct titles to analyze:\" # Changed from \"Product data to analyze:\"\n",
    "    ]\n",
    "\n",
    "    for i, title_str in enumerate(titles_batch):\n",
    "        prompt_parts.append(f\"{i+1}. Title: {title_str}\") # Use title_str directly\n",
    "        \n",
    "    prompt = \"\\n\".join(prompt_parts)\n",
    "\n",
    "    try:\n",
    "        color_attributes_map = gemini_run_query(\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return color_attributes_map\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating color attributes for batch starting with title '{titles_batch[0] if titles_batch else 'N/A'}': {e}\")\n",
    "        # Return a map with empty color lists for all items in the batch in case of error\n",
    "        return {title: {\"colorFamilies\": [], \"colors\": []} for title in titles_batch}\n",
    "\n",
    "# Prepare data for color attribute generation\n",
    "# Use drop_duplicates on title.\n",
    "# Ensure 'description' column might still be needed for display, but not for AI processing here.\n",
    "# If 'description' is not in df_products for other reasons, ensure it's handled or remove from display.\n",
    "if 'description' not in df_products.columns:\n",
    "    df_products['description'] = \"Not available\" \n",
    "\n",
    "df_unique_titles_for_colors = df_products[['title']].drop_duplicates(subset=['title']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Generating Color Attributes (Families and Colors) using generative AI (from Titles only) ===\")\n",
    "batch_size_colors = 200  # Adjust based on typical length of title and token limits\n",
    "final_merged_color_attributes_map = {}\n",
    "\n",
    "print(f\"Processing {len(df_unique_titles_for_colors)} unique titles for color attributes in batches of {batch_size_colors}...\")\n",
    "\n",
    "for i in tqdm(range(0, len(df_unique_titles_for_colors), batch_size_colors)):\n",
    "    print(f\"Processing color attributes batch {i // batch_size_colors + 1}...\")\n",
    "    current_batch_titles_list = []\n",
    "    for idx, row in df_unique_titles_for_colors.iloc[i:i+batch_size_colors].iterrows():\n",
    "        current_batch_titles_list.append(row['title']) # Append only title string\n",
    "    \n",
    "    if not current_batch_titles_list:\n",
    "        continue\n",
    "        \n",
    "    batch_color_map = generate_color_attributes_with_ai(current_batch_titles_list)\n",
    "    \n",
    "    if isinstance(batch_color_map, dict):\n",
    "        final_merged_color_attributes_map.update(batch_color_map)\n",
    "    else:\n",
    "        print(f\"Warning: AI processing for color attributes batch did not return a dictionary for titles starting with '{current_batch_titles_list[0] if current_batch_titles_list else 'N/A'}'.\")\n",
    "        for title_str in current_batch_titles_list:\n",
    "            final_merged_color_attributes_map.setdefault(title_str, {\"colorFamilies\": [], \"colors\": []})\n",
    "    \n",
    "    time.sleep(1) # Respect API rate limits\n",
    "\n",
    "# Assign the generated color attributes to the DataFrame\n",
    "def get_color_families_from_map(title):\n",
    "    return final_merged_color_attributes_map.get(title, {}).get('colorFamilies', [])\n",
    "\n",
    "def get_colors_from_map(title):\n",
    "    return final_merged_color_attributes_map.get(title, {}).get('colors', [])\n",
    "\n",
    "df_products['colorFamilies'] = df_products['title'].apply(get_color_families_from_map)\n",
    "df_products['colors'] = df_products['title'].apply(get_colors_from_map)\n",
    "\n",
    "# Ensure the new columns are lists\n",
    "df_products['colorFamilies'] = df_products['colorFamilies'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "df_products['colors'] = df_products['colors'].apply(lambda x: x if isinstance(x, list) else [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91669602",
   "metadata": {},
   "source": [
    "Let's do a sanity check and look at the first 10 products where `colorInfo` or `colorFamilies` were populated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nColor attribute generation completed! (from Titles only)\")\n",
    "display(\n",
    "    df_products[\n",
    "        (df_products['colorFamilies'].apply(lambda x: isinstance(x, list) and len(x) > 0)) |\n",
    "        (df_products['colors'].apply(lambda x: isinstance(x, list) and len(x) > 0))\n",
    "    ][['title', 'colorFamilies', 'colors']].head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1609319",
   "metadata": {},
   "source": [
    "## Create and populate enhanced product table\n",
    "\n",
    "You now take a look at the df_products dataframe, now that you have enriched your product data with additional attributes such as brand, description, tags, and color information. It contains the original product data along with the enriched fields you have generated using generative AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7091ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_products.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31489bda",
   "metadata": {},
   "source": [
    "You now create a new table in BigQuery to store this enhanced product data. This table will be named `products_enhanced` and will include all the original fields along with the newly generated attributes..\n",
    "\n",
    "The process involves the following key stages to create and populate the `products_enhanced` table in BigQuery:\n",
    "\n",
    "1.  **Table cloning for baseline:**\n",
    "    *   A new table, `products_enhanced`, will be created by cloning the existing `retail.products` table. This will ensure that all original data and the existing schema structure are preserved as a baseline.\n",
    "\n",
    "2.  **Schema extension for enriched attributes:**\n",
    "    *   The schema of the newly cloned `products_enhanced` table will be extended. This involves adding new columns required to store the enriched product attributes (such as brands, description, tags, and color information) that are not present in the original table. The complete target schema, inclusive of these new fields, is typically predefined (e.g., loaded from a Google Cloud Storage file).\n",
    "\n",
    "3.  **Populating extended attributes:**\n",
    "    *   The newly added columns in the `products_enhanced` table will be populated with the enriched data generated within this notebook. This will be achieved by:\n",
    "        *   Preparing the enriched data (e.g., from the `df_products` Pandas DataFrame).\n",
    "        *   Loading this prepared data into a temporary BigQuery table.\n",
    "        *   Executing a `MERGE` operation to update the `products_enhanced` table, matching records by product ID and filling in the values for the extended attributes.\n",
    "\n",
    "4.  **Verification and cleanup:**\n",
    "    *   Post-population, a verification step will be performed by querying a sample of the updated records to ensure data integrity.\n",
    "    *   Any temporary resources, such as the intermediate table used for the merge, will be cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f79c90",
   "metadata": {},
   "source": [
    "### Table cloning and schema extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a424c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "# Ensure BigQuery client is initialized\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# Define table names and schema path\n",
    "TABLE_ENHANCED_ID = f\"{project_id}.retail.products_enhanced\"\n",
    "SOURCE_TABLE_ID = f\"{project_id}.retail.products\"\n",
    "SCHEMA_GCS_PATH = f\"gs://{SCRIPTS_BUCKET}/vaisc-configs/retail_products_schema.json\"\n",
    "\n",
    "print(f\"Target table for creation/update: {TABLE_ENHANCED_ID}\")\n",
    "print(f\"Source table for copy: {SOURCE_TABLE_ID}\")\n",
    "print(f\"Schema GCS path for adding columns: {SCHEMA_GCS_PATH}\")\n",
    "\n",
    "def bq_schema_field_to_ddl_type(field: bigquery.SchemaField) -> str:\n",
    "    \"\"\"Converts a BigQuery SchemaField to its DDL type string.\"\"\"\n",
    "    # Base type determination\n",
    "    if field.field_type == \"RECORD\": # BigQuery client uses \"RECORD\" for STRUCTs\n",
    "        if not field.fields: # Handle empty STRUCT if it can occur\n",
    "            return \"STRUCT<>\"\n",
    "        subfields_ddl = \", \".join([f\"`{sf.name}` {bq_schema_field_to_ddl_type(sf)}\" for sf in field.fields])\n",
    "        base_ddl_type = f\"STRUCT<{subfields_ddl}>\"\n",
    "    else: # Simple types like STRING, INTEGER, FLOAT64, BOOLEAN, TIMESTAMP, DATE etc.\n",
    "        base_ddl_type = field.field_type\n",
    "\n",
    "    # Apply ARRAY if mode is REPEATED\n",
    "    if field.mode == \"REPEATED\":\n",
    "        return f\"ARRAY<{base_ddl_type}>\"\n",
    "    else:\n",
    "        return base_ddl_type\n",
    "\n",
    "try:\n",
    "    # 1. Delete the enhanced table if it exists\n",
    "    client.delete_table(TABLE_ENHANCED_ID, not_found_ok=True)\n",
    "    print(f\"Deleted table {TABLE_ENHANCED_ID} if it existed.\")\n",
    "\n",
    "    # 2. Create the enhanced table as a copy of the source table\n",
    "    print(f\"Executing: copying {SOURCE_TABLE_ID} to {TABLE_ENHANCED_ID}...\")\n",
    "    copy_job = client.copy_table(SOURCE_TABLE_ID, TABLE_ENHANCED_ID)\n",
    "    copy_job.result()  # Wait for the job to complete\n",
    "    print(f\"Successfully created {TABLE_ENHANCED_ID} as a copy of {SOURCE_TABLE_ID}.\")\n",
    "\n",
    "    # 3. Load target schema from Google Cloud Storage\n",
    "    gcs_schema_fields = []\n",
    "    try:\n",
    "        storage_client = storage.Client(project=project_id)\n",
    "        bucket_name, blob_name = SCHEMA_GCS_PATH.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        schema_json_string = blob.download_as_string()\n",
    "        schema_list_of_dicts = json.loads(schema_json_string)\n",
    "        gcs_schema_fields = [bigquery.SchemaField.from_api_repr(field_dict) for field_dict in schema_list_of_dicts]\n",
    "        print(f\"Successfully loaded target schema with {len(gcs_schema_fields)} fields from {SCHEMA_GCS_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading schema from Google Cloud Storage: {e}. Cannot proceed with adding columns.\") \n",
    "        raise # Re-raise to stop execution if schema loading fails\n",
    "\n",
    "    if not gcs_schema_fields:\n",
    "        print(\"Google Cloud Storage Schema was not loaded or is empty, cannot proceed with adding columns.\") \n",
    "    else:\n",
    "               # 4. Get current schema of the newly created table\n",
    "        current_table = client.get_table(TABLE_ENHANCED_ID)\n",
    "        current_field_names = {field.name for field in current_table.schema}\n",
    "        print(f\"Current fields in {TABLE_ENHANCED_ID} after copy: {current_field_names}\")\n",
    "\n",
    "        # 5. Identify and add missing columns\n",
    "        added_columns_count = 0\n",
    "        alter_statements = []\n",
    "        for gcs_field in gcs_schema_fields:\n",
    "            if gcs_field.name not in current_field_names:\n",
    "                column_name = gcs_field.name\n",
    "                column_type_ddl = bq_schema_field_to_ddl_type(gcs_field)\n",
    "                \n",
    "                alter_sql = f\"ALTER TABLE `{TABLE_ENHANCED_ID}` ADD COLUMN `{column_name}` {column_type_ddl}\"\n",
    "                alter_statements.append(alter_sql)\n",
    "                print(f\"  Will add column: `{column_name}` with type {column_type_ddl}\")\n",
    "\n",
    "        if alter_statements:\n",
    "            print(f\"\\nApplying {len(alter_statements)} schema alterations...\")\n",
    "            for i, alter_sql in enumerate(alter_statements):\n",
    "                try:\n",
    "                    print(f\"Executing ALTER statement {i+1}/{len(alter_statements)}: {alter_sql}\")\n",
    "                    alter_job = client.query(alter_sql)\n",
    "                    alter_job.result()  # Wait for completion\n",
    "                    print(f\"Successfully executed: {alter_sql}\")\n",
    "                    added_columns_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error executing ALTER statement '{alter_sql}': {e}\")\n",
    "                    # Optionally, decide if to stop or continue. For now, it continues.\n",
    "            print(f\"Finished applying schema alterations. {added_columns_count} columns potentially added.\")\n",
    "        else:\n",
    "            print(\"No new columns to add. Schema of copied table already contains all columns defined in Google Cloud Storage schema (by name).\")\n",
    "\n",
    "        # Verify final schema\n",
    "        final_table = client.get_table(TABLE_ENHANCED_ID)\n",
    "        final_field_names = {field.name for field in final_table.schema}\n",
    "        print(f\"Final fields in {TABLE_ENHANCED_ID}: {final_field_names}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during table creation or schema update: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab96358",
   "metadata": {},
   "source": [
    "### Data population with extended attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27deb8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the column names of the df_products DataFrame\n",
    "print(df_products.columns)\n",
    "display(df_products.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Ensure BigQuery client is initialized (it should be from previous cells)\n",
    "client = bigquery.Client(project=project_id)\n",
    "\n",
    "# 1. Prepare DataFrame for BigQuery load\n",
    "# Select relevant columns and rename to match BQ schema\n",
    "df_update = df_products[['title', 'description', 'brand', 'tags', 'colorFamilies', 'colors', 'size', 'pattern', 'audience']].copy()\n",
    "df_update.rename(columns={\n",
    "    'brand': 'brands',\n",
    "    'pattern': 'patterns',\n",
    "    'size': 'sizes'\n",
    "}, inplace=True)\n",
    "\n",
    "# Ensure list columns are not NaN, replace with empty list if so\n",
    "# Convert comma-separated string columns ('patterns', 'sizes') to lists\n",
    "def string_to_list(s):\n",
    "    if pd.isna(s) or s == '':\n",
    "        return []\n",
    "    return [item.strip() for item in s.split(',')]\n",
    "\n",
    "df_update['patterns'] = df_update['patterns'].apply(string_to_list)\n",
    "df_update['sizes'] = df_update['sizes'].apply(string_to_list)\n",
    "\n",
    "# Ensure other list columns are indeed lists and handle NaNs\n",
    "for col in ['brands', 'tags', 'colorFamilies', 'colors']:\n",
    "    df_update[col] = df_update[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Ensure 'audience' is a dict, replace None/NaN with default dict structure for BQ STRUCT\n",
    "def ensure_audience_structure(aud):\n",
    "    if pd.isna(aud) or not isinstance(aud, dict):\n",
    "        return {\"genders\": [], \"ageGroups\": []}\n",
    "    # Ensure keys exist\n",
    "    if 'genders' not in aud or not isinstance(aud['genders'], list):\n",
    "        aud['genders'] = []\n",
    "    if 'ageGroups' not in aud or not isinstance(aud['ageGroups'], list):\n",
    "        aud['ageGroups'] = []\n",
    "    return aud\n",
    "\n",
    "df_update['audience'] = df_update['audience'].apply(ensure_audience_structure)\n",
    "\n",
    "\n",
    "print(f\"Prepared DataFrame with {len(df_update)} records for update.\")\n",
    "display(df_update)\n",
    "\n",
    "# Define temporary table ID for loading data\n",
    "TEMP_TABLE_ID = f\"{project_id}.retail.products_temp_update\"\n",
    "\n",
    "# 2. Load DataFrame to a temporary BigQuery table\n",
    "try:\n",
    "    print(f\"Loading data into temporary table: {TEMP_TABLE_ID}...\")\n",
    "    # Delete temp table if it exists from a previous run\n",
    "    client.delete_table(TEMP_TABLE_ID, not_found_ok=True)\n",
    "    print(f\"Deleted temporary table {TEMP_TABLE_ID} if it existed.\")\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True # BQ can infer schema for DataFrames, including dicts as STRUCTs\n",
    "    )\n",
    "    load_job = client.load_table_from_dataframe(\n",
    "        df_update, TEMP_TABLE_ID, job_config=job_config\n",
    "    )\n",
    "    load_job.result()  # Wait for the job to complete\n",
    "    print(f\"Successfully loaded {load_job.output_rows} rows into {TEMP_TABLE_ID}.\")\n",
    "\n",
    "    # 3. Execute MERGE statement to update the target table\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE `{TABLE_ENHANCED_ID}` T\n",
    "    USING `{TEMP_TABLE_ID}` S\n",
    "    ON T.title = S.title\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            T.description = S.description,\n",
    "            T.brands = S.brands,\n",
    "            T.tags = S.tags,\n",
    "            T.colorInfo = STRUCT(\n",
    "                S.colorFamilies AS colorFamilies, \n",
    "                S.colors AS colors\n",
    "            ),\n",
    "            T.patterns = S.patterns,\n",
    "            T.sizes = S.sizes,\n",
    "            T.audience = STRUCT(\n",
    "                S.audience.genders AS genders,\n",
    "                S.audience.ageGroups AS ageGroups\n",
    "            )\n",
    "    \"\"\"\n",
    "    print(\"\\nExecuting MERGE statement...\")\n",
    "    print(merge_query)\n",
    "    merge_job = client.query(merge_query)\n",
    "    merge_job.result()  # Wait for the job to complete\n",
    "    print(f\"MERGE statement completed. {merge_job.num_dml_affected_rows} rows affected in {TABLE_ENHANCED_ID}.\")\n",
    "\n",
    "finally:\n",
    "    # 4. Clean up the temporary table\n",
    "    client.delete_table(TEMP_TABLE_ID, not_found_ok=True)\n",
    "    print(f\"\\nTemporary table {TEMP_TABLE_ID} deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643bcbeb",
   "metadata": {},
   "source": [
    "Let's perform a final sanity check and look at the first 100 products loaded from the BigQuery table `products_enhanced` where at least one of the enriched fields is populated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53769ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Verification query\n",
    "print(\"\\nFetching a sample of updated records from products_enhanced...\")\n",
    "verification_query = f\"\"\"\n",
    "SELECT \n",
    "    id, \n",
    "    description, \n",
    "    brands, \n",
    "    tags,\n",
    "    colorInfo.colorFamilies AS colorFamilies,\n",
    "    colorInfo.colors AS colors,\n",
    "    patterns,\n",
    "    sizes,\n",
    "    audience.genders AS audience_genders,\n",
    "    audience.ageGroups AS audience_ageGroups\n",
    "FROM `{TABLE_ENHANCED_ID}`\n",
    "WHERE description IS NOT NULL \n",
    "    OR ARRAY_LENGTH(brands) > 0 \n",
    "    OR ARRAY_LENGTH(tags) > 0 \n",
    "    OR ARRAY_LENGTH(colorInfo.colorFamilies) > 0 \n",
    "    OR ARRAY_LENGTH(colorInfo.colors) > 0\n",
    "    OR ARRAY_LENGTH(patterns) > 0\n",
    "    OR ARRAY_LENGTH(sizes) > 0\n",
    "    OR ARRAY_LENGTH(audience.genders) > 0\n",
    "    OR ARRAY_LENGTH(audience.ageGroups) > 0\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "df_verified = client.query(verification_query).to_dataframe()\n",
    "display(df_verified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5efe13",
   "metadata": {},
   "source": [
    "# Verification of data enriched catalog\n",
    "Now that you have populated the `products_enhanced` table with enriched product data, it is crucial to verify the integrity and accuracy of the enriched attributes. This step ensures that the data meets your expectations and is ready for use in search and recommendation systems.\n",
    "Use the Vertex AI Search for commerce validation tool to perform this verification. The validation tool allows you to check the consistency and correctness of the enriched data against the expected schema and values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e20ea8",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "You now load the enriched product data from the `products_enhanced` table into the Vertex AI Search for commerce catalog. For testing purposes, you use Branch 2 of the catalog. This allows you to validate the enriched data without affecting the production catalog.\n",
    "\n",
    "You use a Python script to load the data. You can also use the [Vertex AI Search for commerce console](https://console.cloud.google.com/ai/retail/catalogs/default_catalog/data/catalog) to load the data, but using a Python script allows you to automate the process and easily repeat it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567cee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import retail_v2\n",
    "from google.cloud.retail_v2.types import ImportProductsRequest, ProductInputConfig, BigQuerySource, ImportErrorsConfig\n",
    "from google.cloud import storage\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Define Branch 2 for testing enriched data\n",
    "BRANCH_2 = f\"projects/{project_id}/locations/global/catalogs/default_catalog/branches/2\"\n",
    "ENHANCED_TABLE_FULL_ID = f\"{project_id}.retail.products_enhanced\"\n",
    "\n",
    "print(f\"Loading enriched data from {ENHANCED_TABLE_FULL_ID} into Branch 2: {BRANCH_2}\")\n",
    "\n",
    "def load_products_to_branch_from_bq(branch_name, table_id, reconciliation_mode=\"FULL\"):\n",
    "    \"\"\"\n",
    "    Load products from BigQuery table into a specific catalog branch.\n",
    "    \n",
    "    Args:\n",
    "        branch_name: Target branch (e.g., \"projects/.../branches/2\")\n",
    "        table_id: BigQuery table ID (e.g., \"products_enhanced\" or \"retail.products_enhanced\")\n",
    "        reconciliation_mode: \"INCREMENTAL\" or \"FULL\"\n",
    "    \"\"\"\n",
    "    # Initialize the ProductService client\n",
    "    client = retail_v2.ProductServiceClient()\n",
    "\n",
    "    # Parse dataset_id and table_id_short from the full table_id string if needed\n",
    "    # If table_id is in the form \"project.dataset.table\", split it\n",
    "    # project_id is assumed to be the global project_id for the BQ source\n",
    "    if \".\" in table_id:\n",
    "        parts = table_id.split(\".\")\n",
    "        if len(parts) == 3: # project.dataset.table\n",
    "            _project_id_from_table_str, dataset_id, table_id_short = parts\n",
    "        elif len(parts) == 2: # dataset.table\n",
    "            dataset_id, table_id_short = parts\n",
    "        else: # table (assuming default dataset 'retail')\n",
    "            dataset_id = \"retail\" \n",
    "            table_id_short = table_id\n",
    "    else: # table (assuming default dataset 'retail')\n",
    "        dataset_id = \"retail\"\n",
    "        table_id_short = table_id\n",
    "\n",
    "    bq_source = BigQuerySource(\n",
    "        project_id=project_id, # Use the global project_id for the BQ source\n",
    "        dataset_id=dataset_id,\n",
    "        table_id=table_id_short,\n",
    "        data_schema=\"product\" # Specifies that the BQ table schema matches the Retail API product schema\n",
    "    )\n",
    "\n",
    "    input_config = ProductInputConfig(\n",
    "        big_query_source=bq_source\n",
    "    )\n",
    "\n",
    "    errors_config = ImportErrorsConfig(\n",
    "        gcs_prefix=f\"gs://{SCRIPTS_BUCKET}/retail_import_errors/\" # GCS path for error logs\n",
    "    )\n",
    "\n",
    "    request = ImportProductsRequest(\n",
    "        parent=branch_name,\n",
    "        input_config=input_config,\n",
    "        errors_config=errors_config,\n",
    "        reconciliation_mode=reconciliation_mode,\n",
    "        notification_pubsub_topic=None # Set to a Pub/Sub topic for notifications if needed\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print(f\"Starting import operation for branch: {branch_name}\")\n",
    "        print(f\"Source table: {project_id}.{dataset_id}.{table_id_short}\")\n",
    "        print(f\"Reconciliation mode: {reconciliation_mode}\")\n",
    "        operation = client.import_products(request=request)\n",
    "        print(f\"Import operation started. Operation name: {operation.operation.name}\")\n",
    "        print(\"Waiting for import operation to complete (timeout: 1800s)...\")\n",
    "        result = operation.result(timeout=1800) # Wait for the LRO to complete\n",
    "        print(\"Import operation completed successfully!\")\n",
    "        print(f\"Operation result: {result}\")\n",
    "        return operation.operation.name, result\n",
    "    except Exception as e:\n",
    "        print(f\"Error during import operation: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def check_import_operation_status(operation_name):\n",
    "    \"\"\"\n",
    "    Check the status of an import operation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.cloud import retail_v2\n",
    "        client = retail_v2.ProductServiceClient()\n",
    "        # The operations_client is part of the ProductServiceClient's transport layer\n",
    "        operation = client.transport.operations_client.get_operation(\n",
    "            name=operation_name\n",
    "        )\n",
    "        print(f\"Operation status: {operation}\")\n",
    "        return operation\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking operation status: {e}\")\n",
    "        return None\n",
    "\n",
    "# Execute the data loading\n",
    "print(\"=== Loading Enhanced Product Data to Branch 2 ===\")\n",
    "\n",
    "# Load data with INCREMENTAL mode. Change to \"FULL\" for a full replacement if needed.\n",
    "# The table_id here refers to the table name within the 'retail' dataset of the current project_id.\n",
    "operation_name, result = load_products_to_branch_from_bq(\n",
    "    branch_name=BRANCH_2,\n",
    "    table_id=\"products_enhanced\", # This will be resolved to project_id.retail.products_enhanced\n",
    "    reconciliation_mode=\"INCREMENTAL\"\n",
    ")\n",
    "\n",
    "if operation_name:\n",
    "    print(f\"\\nImport operation to Branch 2 initiated successfully.\")\n",
    "    print(f\"Operation name: {operation_name}\")\n",
    "    if result: # Result is available if operation completed synchronously or LRO finished\n",
    "        print(f\"Import result details: {result}\")\n",
    "else:\n",
    "    print(\"Import operation to Branch 2 failed to initiate or complete. Please check the error messages above.\")\n",
    "\n",
    "# Optional: Check operation status again if needed, especially if it was a long-running operation.\n",
    "if operation_name:\n",
    "    print(\"\\n=== Checking Final Operation Status (if LRO was still pending) ===\")\n",
    "    final_status = check_import_operation_status(operation_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11605b",
   "metadata": {},
   "source": [
    "## Validation\n",
    "Now that you have loaded the enriched product data into the Vertex AI Search for commerce catalog, you can proceed with the validation process. You use both automated and manual validation techniques. The validation tool helps you ensure that the enriched data meets the expected schema and values, and is ready for use in search and recommendation systems.\n",
    "\n",
    "First, you need to identify the products that you have enriched and loaded into the catalog. Compare the results of catalog search performed against Branch 0 (original product data) and Branch 2 (enriched product data). You then identify search strings for relevant titles for you to perform the search with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fdc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 50 records from df_update where any extended attributes are not empty\n",
    "non_empty_mask = (\n",
    "    df_update['tags'].apply(lambda x: isinstance(x, list) and len(x) > 0) |\n",
    "    df_update['patterns'].apply(lambda x: isinstance(x, list) and len(x) > 0) |\n",
    "    df_update['sizes'].apply(lambda x: isinstance(x, list) and len(x) > 0) |\n",
    "    df_update['brands'].apply(lambda x: isinstance(x, list) and len(x) > 0) |\n",
    "    df_update['colorFamilies'].apply(lambda x: isinstance(x, list) and len(x) > 0) |\n",
    "    df_update['colors'].apply(lambda x: isinstance(x, list) and len(x) > 0) |\n",
    "    df_update['audience'].apply(lambda x: isinstance(x, dict) and (x.get('genders') or x.get('ageGroups')) and (len(x.get('genders', [])) > 0 or len(x.get('ageGroups', [])) > 0))\n",
    ")\n",
    "display(df_update[non_empty_mask].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfcb3bd",
   "metadata": {},
   "source": [
    "While your results may very, for the products in this lab, you might find that the following search strings return the enriched products:\n",
    "- \"tote\"\n",
    "- \"bag\"\n",
    "- \"android phone\"\n",
    "\n",
    "To have visually appealing results, use the [Vertex AI Search for commerce console](https://console.cloud.google.com/ai/retail/catalogs/default_catalog/evaluate/search) to perform the search. You can also use the Python script to perform the search, but using the console will allow you to easily visualize the results and compare them.\n",
    "\n",
    "- Navigate to the [Vertex AI Search for commerce console](https://console.cloud.google.com/ai/retail/catalogs/default_catalog/evaluate/search)\n",
    "- Select `Branch 2` from the dropdown `Select catalog branch`\n",
    "- Enter the search string in the `Search query` field, use one of the search strings identified above\n",
    "- Click the `Search` button\n",
    "\n",
    "You should see the enriched products in the search results. To validate the advanced search attributes have been populated correctly, `select all` under `Facets` drop-down control. As you will remember, you enabled site-wide controls to make all attributes are searchable, indexable and retrievable. Therefore, facets will be populated with the values from the enriched product data.\n",
    "\n",
    "You should see something like this in the search results:\n",
    "![Enhanced search results](https://raw.githubusercontent.com/volenin/vaisc-csb/refs/heads/main/notebooks/img/data_management_enchanced_results.png)\n",
    "\n",
    "Now run the same same search against Branch 0 (original product data) and compare the results. You should see that the enriched products are not present in the search results for Branch 0, as they were not loaded into that branch.\n",
    "\n",
    "![Basic search results](https://raw.githubusercontent.com/volenin/vaisc-csb/refs/heads/main/notebooks/img/data_management_basic_results.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115b397",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this lab, you successfully demonstrated how to manage and enrich product data using Google Cloud services, particularly focusing on Vertex AI and BigQuery. You covered the following key steps:\n",
    "-   **Data analysis**: You analyzed the existing product data to identify gaps and opportunities for enrichment.\n",
    "-   **Data enrichment**: You utilized generative AI models to extract structured information from product titles, generating attributes such as brand names, descriptions, tags, and color information.\n",
    "-   **Data population**: You created a new BigQuery table to store the enriched product data, ensuring that all original fields and newly generated attributes were included.\n",
    "-   **Data validation**: You validated the enriched product data using the Vertex AI Search for commerce validation tool, ensuring that the data met your expectations and was ready for use in search and recommendation systems.\n",
    "\n",
    "**Areas for improvement**:\n",
    "-   **Image analysis**: In this lab, you focused on text data for simplicity. Future iterations could incorporate image analysis to extract additional attributes from product images, enhancing the catalog further.\n",
    "-   **Attributes cleansing**: While you consolidated tags and extracted colors, further cleansing and normalization of attributes could improve data quality. For example, ensuring consistent naming conventions for brands and colors, standardizing sizing, genders, and audience attributes.\n",
    "\n",
    "**Operationalization**:\n",
    "In this lab, you demonstrated an approach to improve product data quality and searchability in an iterative manner. The steps you followed can be operationalized in a production environment. One of the easiest ways to do this is to use the [BigQuery Pipelines](https://cloud.google.com/bigquery/docs/pipelines-introduction), as it supports direct execution of notebooks. Another approach is to transform the notebook into a Python script or PySpark jobs that can be scheduled to run periodically through Cloud Scheduler or BigQuery scheduled queries.\n",
    "\n",
    "**Role of generative AI**:\n",
    "Generative AI played a crucial role in this lab by enabling you to extract structured information from unstructured text data. Not only that, the entire codebase in this notebook was generated using Gemini model, which demonstrates the power of generative AI in automating and enhancing data management tasks. The code operationalization can also be done with help of generative AI.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
